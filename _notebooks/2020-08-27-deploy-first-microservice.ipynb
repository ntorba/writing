{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy First Microservice\n",
    "> Deploy your first microservice on kubernetes!\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [kubernetes, docker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-reqs\n",
    "* [Build a Seldon-core Microservice]()\n",
    "* [Launch a local kubernetes cluster](https://ntorba.github.io/writing/jupyter/2020/07/17/local-kubernetes.html)\n",
    "    * or any other kubernetes cluster readily available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* Deploy our docker image from [Build a Seldon-core Microservice](https://ntorba.github.io/writing/seldon-core/docker/2020/07/30/first-seldon-core-microservice.html) onto kubernetes!\n",
    "\n",
    "#### Steps\n",
    "1. Define SeldonDeployment yaml file \n",
    "2. `kubectl apply` SeldonDeployment to the kubernetes cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SeldonDeployment yaml file\n",
    "First, I show a completed seldon deployment configuration file. Under this file I walk through some of the important details to take take note of. \n",
    "In this example, we are deploying a custom python component. Seldon-core also provides [pre-packaged model servers](https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html), which allow you to get models up faster, but the concepts don't transfer as well to more complex inference graphs with other components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier/sklearn_iris_deployment.yaml\n",
    "#hide_output\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-deployment-example\n",
    "spec:\n",
    "  name: sklearn-iris-deployment\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/sklearn-iris:0.1\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          name: sklearn-iris-classifier\n",
    "    graph:\n",
    "      children: []\n",
    "      endpoint:\n",
    "        type: REST\n",
    "      name: sklearn-iris-classifier\n",
    "      type: MODEL\n",
    "    name: sklearn-iris-predictor\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important notes about the deployment config: \n",
    "* apiVersion: this sends out request to the appropriate endpoint of the kubernets api, which was installed by helm earlier in this tutorial\n",
    "    * learn more about this in our [kubernetes posts]()\n",
    "* kind: tells Kubernetes what kind of resource to create.\n",
    "    * Because we installed seldon-core on our cluster, it recognizes SeldonDeployment as a custom resource\n",
    "* metadata: add labels, like name, to the deployment\n",
    "* spec: \n",
    "    * predictors: this is a list of predictors to deploy. It is a list because you have the option to create multiple inference graphs in the same spec. This is useful for things like Canary deployment, where you only want a new graph to recieve a small percentage of traffic\n",
    "        * componentSpecs: add information about the containers that need to be pulled to create our graph. In our case, we only need a single containe to serve our model. If we were creating a more complex inference graph (maybe with a transformer, router, and another model, then we would need to include the docker containers that house them in this section)\n",
    "        * graph: this is where you define the flow of components. This is easy in our case, there is only one component so we define one endpoint with no children. If there were more compnoents, we would fill out the children componenets in the children attriubte of the head of the graph. Seldon graphs are built implicitly through the use of the children attribute of each node in the graph. \n",
    "        \n",
    "There is one last step to deploy our graph, we must push our docker container to a registry! I am running a local registry with my kind cluster, thanks to the script given [here](https://kind.sigs.k8s.io/docs/user/local-registry/). You can also push to DockerHub as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our docker image in a registry, it is available to our cluster, so we can deploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f iris_classifier/sklearn_iris_deployment.yaml\n",
    "from time import sleep\n",
    "sleep(5) # give the clsuter some to get the deployment running before executing the rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-deployment-example \\\n",
    "                                 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment is ready, you will need to port-forward the pod to your localhost in order check the request. That can be done wiht kubectl port-forward command \n",
    "```bash \n",
    "kubectl port-forward $(kubectl get pods -l seldon-app=seldon-deployment-example-sklearn-iris-predictor -o jsonpath='{.items[0].metadata.name}') 9000:9000\n",
    "```\n",
    "\n",
    "You must run this command in a separate window because it will need to run while we curl the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grpc \n",
    "from seldon_core.proto import prediction_pb2\n",
    "from seldon_core.proto import prediction_pb2_grpc\n",
    "\n",
    "\n",
    "### Test REST endpoint\n",
    "res = !curl -s http://localhost:9000/predict -H \"Content-Type: application/json\" -d '{\"data\":{\"ndarray\":[[5.964,4.006,2.081,1.031]]}}'\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup\n",
    "!kubectl delete -f sklearn_iris_deployment.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "In this quick example, we scratched the surface of seldon-core by deploying a simple model endpoint on kubernetes. \n",
    "If you are hungry for more, chech out more of the posts in the [Seldon Super Series](). There, you can find notebooks similar to this that deploy more complex inference graphs, or dive into the underlying kubernetes concepts that seldon runs on top of! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next Up\n",
    "* other seldon components \n",
    "* seldon graph construction \n",
    "* multi-component inference graph\n",
    "* operators and custom resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
