{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Seldon-core Microservice\n",
    "> Get an ML endpoint up and running!\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [seldon-core, docker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reqs\n",
    "* docker\n",
    "    * If you are coming from [Launch a local kubernetes cluster](https://ntorba.github.io/writing/jupyter/2020/07/17/local-kubernetes.html), you are good to follow this example. If not, you can quickly follow that post before running the example here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* Prepare a docker image with seldon-core for deployment on kubernetes\n",
    "\n",
    "#### Steps\n",
    "1. Define a seldon python component\n",
    "2. Build docker image\n",
    "3. Run a container to test code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Python Component\n",
    "I'm taking this example code directly from [seldon-core irisClassifier example](https://github.com/SeldonIO/seldon-core/blob/master/examples/models/sklearn_iris/sklearn_iris.ipynb). \n",
    "First, we train a model based on the iris dataset included in the sklearn package, then we serve that trained model in the seldon endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: iris_classifier: File exists\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!mkdir iris_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/train_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/train_iris.py\n",
    "#collapse_show\n",
    "#hide_output\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "OUTPUT_FILE = \"iris_classifier/IrisClassifier.sav\"\n",
    "\n",
    "\n",
    "print(\"Loading iris data set...\")\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "p = Pipeline([(\"clf\", clf)])\n",
    "print(\"Training model...\")\n",
    "p.fit(X, y)\n",
    "print(\"Model trained!\")\n",
    "\n",
    "print(f\"Saving model in {OUTPUT_FILE}\")\n",
    "joblib.dump(p, OUTPUT_FILE)\n",
    "print(\"Model saved!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iris data set...\n",
      "Dataset loaded!\n",
      "Training model...\n",
      "Model trained!\n",
      "Saving model in iris_classifier/IrisClassifier.sav\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!python iris_classifier/train_iris.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the seldon python component that will be used to serve the model. \n",
    "Seldon has a few [components](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/python_component.html). In this example, we only use the Model component. Seldon components hold the logic that will be implanted into the serving endpoint that seldon creates. The model component must have a predict function, which is called when the future endpoint is hit. \n",
    "The reason seldon is so useful is because this is the only python code we need to write to serve this model. Seldon provides the rest of the logic, which puts this component into a web server, to serve the model.\n",
    "\n",
    "An important note about this section is that you'lll see the file is named `IrisClassifier.py`, which is camelcased. This is important, and you should not change this. The file name and the python component class name **must match**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/IrisClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/IrisClassifier.py\n",
    "#collapse_show\n",
    "#hide_output\n",
    "import joblib\n",
    "\n",
    "class IrisClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = joblib.load('IrisClassifier.sav')\n",
    "\n",
    "    def predict(self,X,features_names):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Image\n",
    "After defining a python component, there are two ways to create the docker image necessary for deployment. \n",
    "* [define a Dockerfile](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/python_wrapping_docker.html) which launches the seldon microservice\n",
    "* use [s2i](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/wrappers/s2i.html) to build the image directly from source code. \n",
    "\n",
    "I prefer manually defining a Dockerfile because it provides more control over the process. However, s2i is a great tool that works just as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write requirements.txt \n",
    "We must write a requirements.txt library with all requirements for the docker image listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/requirements.txt\n",
    "#hide_output\n",
    "sklearn\n",
    "seldon-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Dockerfile\n",
    "The Dockerfile follows the example provided [here](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/python_wrapping_docker.html). \n",
    "We start from the python:3.7-slim base image, copy the code from the current directory, which includes the python component we defined earlier, install requirements, then expose port 5000 for the microservice to run. \n",
    "Next, we define seldon specific variables. \n",
    "* MODEL_NAME must match the python file name (which also much match the python component class name). \n",
    "* API_TYPE can be either REST or GRPC.\n",
    "* SERVICE_TYPE is the type of seldon component. MODEL for this example. (explore the other seldon components [here]()\n",
    "* PERSISTENCE: 0 or 1. Defaults to 0. If it is set to 1, the component class will be periodically persisted to reis. This s unnecessary for our case because the component class will not change.\n",
    "    * this would be more pertinent for components like routers, which can have updating states for long running jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/Dockerfile\n",
    "#collapse_show\n",
    "#hide_output\n",
    "FROM python:3.7-slim\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "RUN pip install -r requirements.txt\n",
    "EXPOSE 5000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_NAME IrisClassifier \n",
    "ENV API_TYPE REST\n",
    "ENV SERVICE_TYPE MODEL \n",
    "ENV PERSISTENCE 0\n",
    "\n",
    "# seldon-core-microservice is a command line tool installed with the seldon-core python libray. You can use this locally as well!\n",
    "CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this example, let's build and run the docker image! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker Build\n",
    "Pass the iris_classifier dir where the image guts live, then pass a -t to tag the image with a name referring to your preferred docker image repository (I'm running on locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/10 : FROM python:3.7-slim\n",
      " ---> b386e7420fc3\n",
      "Step 2/10 : COPY . /app\n",
      " ---> 4ff1fc2d09e5\n",
      "Step 3/10 : WORKDIR /app\n",
      " ---> Running in 0e5b783b9df2\n",
      "Removing intermediate container 0e5b783b9df2\n",
      " ---> 840bd996fe26\n",
      "Step 4/10 : RUN pip install -r requirements.txt\n",
      " ---> Running in 6f1af0205271\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting seldon-core\n",
      "  Downloading seldon_core-1.2.2-py3-none-any.whl (108 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting Flask<2.0.0\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting Flask-cors<4.0.0\n",
      "  Downloading Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting prometheus-client<0.9.0,>=0.7.1\n",
      "  Downloading prometheus_client-0.8.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting Flask-OpenTracing<1.2.0,>=1.1.0\n",
      "  Downloading Flask-OpenTracing-1.1.0.tar.gz (8.2 kB)\n",
      "Collecting opentracing<2.4.0,>=2.2.0\n",
      "  Downloading opentracing-2.3.0.tar.gz (48 kB)\n",
      "Collecting requests<3.0.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting protobuf<4.0.0\n",
      "  Downloading protobuf-3.12.4-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting gunicorn<20.1.0,>=19.9.0\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "Collecting PyYAML<5.4\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting jsonschema<4.0.0\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting jaeger-client<4.4.0,>=4.1.0\n",
      "  Downloading jaeger-client-4.3.0.tar.gz (81 kB)\n",
      "Collecting numpy<2.0.0\n",
      "  Downloading numpy-1.19.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting grpcio<2.0.0\n",
      "  Downloading grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting flatbuffers<2.0.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting minio<6.0.0,>=4.0.9\n",
      "  Downloading minio-5.0.10-py2.py3-none-any.whl (75 kB)\n",
      "Collecting grpcio-opentracing<1.2.0,>=1.1.4\n",
      "  Downloading grpcio_opentracing-1.1.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from seldon-core->-r requirements.txt (line 3)) (47.3.1)\n",
      "Collecting redis<4.0.0\n",
      "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting Six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.16.0.tar.gz (108 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting threadloop<2,>=1\n",
      "  Downloading threadloop-1.0.2.tar.gz (4.9 kB)\n",
      "Collecting thrift\n",
      "  Downloading thrift-0.13.0.tar.gz (59 kB)\n",
      "Collecting tornado>=4.3\n",
      "  Downloading tornado-6.0.4.tar.gz (496 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting configparser\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: sklearn, Flask-OpenTracing, opentracing, PyYAML, jaeger-client, pyrsistent, threadloop, thrift, tornado\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=f35bba8878dbd98e914c5a5adaf27a0f1d876a5edecb8bc12a6fc820567cb767\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "  Building wheel for Flask-OpenTracing (setup.py): started\n",
      "  Building wheel for Flask-OpenTracing (setup.py): finished with status 'done'\n",
      "  Created wheel for Flask-OpenTracing: filename=Flask_OpenTracing-1.1.0-py3-none-any.whl size=9070 sha256=a5523f52ee6f8ac6d14676c4ae4366329ed5d0de51efad438d62490954d8b0a0\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/22/cd/ccb93fa68f4a01fb6c10082f97bcb2af9eb8e43565ce38a292\n",
      "  Building wheel for opentracing (setup.py): started\n",
      "  Building wheel for opentracing (setup.py): finished with status 'done'\n",
      "  Created wheel for opentracing: filename=opentracing-2.3.0-py3-none-any.whl size=51347 sha256=af662d9bfa99e590a3ea577797b851120b02e5aedd922a24daaa23c38c37d2c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/c5/4b/b030afc055aa78698cd96eb4b168b7f91bd9254191bf4e9f9f\n",
      "  Building wheel for PyYAML (setup.py): started\n",
      "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=13c5467ee2b39b34ccdcb5c19c4b242d07b2524ea74a5ffa8973edd82eb512c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "  Building wheel for jaeger-client (setup.py): started\n",
      "  Building wheel for jaeger-client (setup.py): finished with status 'done'\n",
      "  Created wheel for jaeger-client: filename=jaeger_client-4.3.0-py3-none-any.whl size=64291 sha256=10455b57709cf0e0639dbc033320426fe2f6b55bf3861f06bc4d6dd1d010bcf1\n",
      "  Stored in directory: /root/.cache/pip/wheels/4b/b9/d9/efe18893b02a4bc5abb68e0174d4ab10147f7f184dd170758e\n",
      "  Building wheel for pyrsistent (setup.py): started\n",
      "  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.16.0-cp37-cp37m-linux_x86_64.whl size=56582 sha256=e91c522596849dad9a07d485e0370d16ab53d3cbc0bf0a3119402ac2b5f5be5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/52/11/f0920f95c23ed7d2d0b05f2b7b2f4509e87a20cfe8ea43d987\n",
      "  Building wheel for threadloop (setup.py): started\n",
      "  Building wheel for threadloop (setup.py): finished with status 'done'\n",
      "  Created wheel for threadloop: filename=threadloop-1.0.2-py3-none-any.whl size=3423 sha256=fb54794ec4429b21ce2ec715b80c85e4c6acb18f43a5c09dad6de997ab9f90ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/93/e3/037c2555d98964d9ca537dabb39827a2b72470a679b5c0de37\n",
      "  Building wheel for thrift (setup.py): started\n",
      "  Building wheel for thrift (setup.py): finished with status 'done'\n",
      "  Created wheel for thrift: filename=thrift-0.13.0-py3-none-any.whl size=154885 sha256=286ebdc004bd104a372a362985f877758255719a3128b7b4c712a7ce3af932bb\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/35/5a/19f5dadf91f62bd783aaa8385f700de9bc14772e09ab0f006a\n",
      "  Building wheel for tornado (setup.py): started\n",
      "  Building wheel for tornado (setup.py): finished with status 'done'\n",
      "  Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=415150 sha256=87125dcb289d087fca2d318e4df34ee088cc7c5351a0f31862c1c9c8b171df2a\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348\n",
      "Successfully built sklearn Flask-OpenTracing opentracing PyYAML jaeger-client pyrsistent threadloop thrift tornado\n",
      "Installing collected packages: threadpoolctl, numpy, scipy, joblib, scikit-learn, sklearn, itsdangerous, Werkzeug, MarkupSafe, Jinja2, click, Flask, Six, Flask-cors, prometheus-client, opentracing, Flask-OpenTracing, urllib3, chardet, certifi, idna, requests, protobuf, gunicorn, PyYAML, pyrsistent, zipp, importlib-metadata, attrs, jsonschema, tornado, threadloop, thrift, jaeger-client, grpcio, flatbuffers, pytz, python-dateutil, configparser, minio, grpcio-opentracing, redis, seldon-core\n",
      "Successfully installed Flask-1.1.2 Flask-OpenTracing-1.1.0 Flask-cors-3.0.8 Jinja2-2.11.2 MarkupSafe-1.1.1 PyYAML-5.3.1 Six-1.15.0 Werkzeug-1.0.1 attrs-19.3.0 certifi-2020.6.20 chardet-3.0.4 click-7.1.2 configparser-5.0.0 flatbuffers-1.12 grpcio-1.30.0 grpcio-opentracing-1.1.4 gunicorn-20.0.4 idna-2.10 importlib-metadata-1.7.0 itsdangerous-1.1.0 jaeger-client-4.3.0 joblib-0.16.0 jsonschema-3.2.0 minio-5.0.10 numpy-1.19.1 opentracing-2.3.0 prometheus-client-0.8.0 protobuf-3.12.4 pyrsistent-0.16.0 python-dateutil-2.8.1 pytz-2020.1 redis-3.5.3 requests-2.24.0 scikit-learn-0.23.2 scipy-1.5.2 seldon-core-1.2.2 sklearn-0.0 threadloop-1.0.2 threadpoolctl-2.1.0 thrift-0.13.0 tornado-6.0.4 urllib3-1.25.10 zipp-3.1.0\n",
      "\u001b[91mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 6f1af0205271\n",
      " ---> 0dc34bb5fbcb\n",
      "Step 5/10 : EXPOSE 5000\n",
      " ---> Running in b5eb29d44f0a\n",
      "Removing intermediate container b5eb29d44f0a\n",
      " ---> 917767077f48\n",
      "Step 6/10 : ENV MODEL_NAME IrisClassifier\n",
      " ---> Running in e4774d29be83\n",
      "Removing intermediate container e4774d29be83\n",
      " ---> 3c496258db36\n",
      "Step 7/10 : ENV API_TYPE REST\n",
      " ---> Running in be32460a5412\n",
      "Removing intermediate container be32460a5412\n",
      " ---> 7f22bdd98919\n",
      "Step 8/10 : ENV SERVICE_TYPE MODEL\n",
      " ---> Running in d6b8303554c4\n",
      "Removing intermediate container d6b8303554c4\n",
      " ---> 90ba6accec5f\n",
      "Step 9/10 : ENV PERSISTENCE 0\n",
      " ---> Running in 5d7de4dc3ce0\n",
      "Removing intermediate container 5d7de4dc3ce0\n",
      " ---> f1865fdf1b32\n",
      "Step 10/10 : CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE\n",
      " ---> Running in 29a17b3da061\n",
      "Removing intermediate container 29a17b3da061\n",
      " ---> 33de3b5555b6\n",
      "Successfully built 33de3b5555b6\n",
      "Successfully tagged localhost:5000/iris_ex:latest\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!docker build iris_classifier/ -t localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Image\n",
    "You can test your newly created image by running the image and hitting the endpoint. \n",
    "You may ask yourself at this point, \"if I have a working docker image, what do I need kubernetes for?\" \n",
    "This is a great question. For simple use cases, this docker image itself is all you need, and you could run it as a standalone service. If the load is small and you can run it without any load balancing functionalities, you are good to go. \n",
    "However, kubernetes is a container orchestration engine. That means it is built to handle complex containerized applications and will make your life much easier if you need to handle more complex operations for applications that need to serve on a large scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d88f1163a71622fc2b67f33b8af4e95c2c8dafa9da43e2fe8c06e4322b7591c\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!docker run --name \"iris_predictor\" -d --rm -p 5001:5000 localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also remove the -d argument from the above command and run this command in a separate window to see the log output while sending requests to the endpoint. Test the endpoint with the curl below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grpc \n",
    "from seldon_core.proto import prediction_pb2\n",
    "from seldon_core.proto import prediction_pb2_grpc\n",
    "\n",
    "\n",
    "### Test Rest Endpoint\n",
    "!curl -s http://localhost:5001/predict -H \"Content-Type: application/json\" -d '{\"data\":{\"ndarray\":[[5.964,4.006,2.081,1.031]]}}'\n",
    "\n",
    "\n",
    "### Test GRPC Endpoint\n",
    "# data = np.array([[5.964,4.006,2.081,1.031]])\n",
    "\n",
    "# datadef = prediction_pb2.DefaultData(\n",
    "#     tensor=prediction_pb2.Tensor(shape=data.shape, values=data.flatten())\n",
    "# )\n",
    "# request = prediction_pb2.SeldonMessage(data=datadef)\n",
    "# with grpc.insecure_channel(\"localhost:5001\") as channel:\n",
    "#     stub = prediction_pb2_grpc.ModelStub(channel)\n",
    "#     response = stub.Predict(request=request)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see successful output, you have your first seldon-core microservice up and running, congrats! \n",
    "Use the next command to stop the running docker container: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container rm iris_predictor --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are like me, you are a bit surprised at just how easy it was to create this. That is the power of seldon. \n",
    "\n",
    "You may be wondering, what about kubernetes? Didn't your first post in this series say that you must have access to a cluster? That is still correct, and you will need that cluster in the following posts. I just like to start with this piece because it is straightforward and a rewarding way to start. \n",
    "\n",
    "At this stage you are prepared to move onto any of these posts: \n",
    "* [Deploy First Microservice]()\n",
    "    * This post is for those excited to deploy your new docker image on kubernetes! \n",
    "* [Build Various Seldon-core microservices]()\n",
    "    * This post is for those more interested in learning about the other seldon components beside models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP HERE\n",
    "* Once the docker image is created, I will create a new post that is specific to launching on kubernetes\n",
    "* Getting a working docker image is half the battle, and this can be a big success for users\n",
    "    * this is also a great jumping off point to instead say, want to make a more complex inference graph before kubernetes? or want to go learn about other python components before moving on to kubernetes?\n",
    "* 1) building docker, 2) launch on kubernetes\n",
    "    * in the light of mouldarity, I am better off keeping these steps separate from each other so users can focus on the docker and seldon stuff if they like\n",
    "    * this has the other advantage that if users struggle with getting a local k8s cluster up, they can still leaving the tutorials feelings they have gained a good understanding of seldon because they have their nice docker images they built\n",
    "\n",
    "First, let's take down the running docker container:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
