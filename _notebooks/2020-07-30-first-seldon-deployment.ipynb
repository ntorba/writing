{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Seldon Deployment\n",
    "> Get an ML endpoint up and running on your cluster!\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [kubernetes, docker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reqs\n",
    "* access to kubernetes cluster \n",
    "    * If you are coming from [Launch a local kubernetes cluster](https://ntorba.github.io/writing/jupyter/2020/07/17/local-kubernetes.html), you are good to follow this example. If not, you can quickly follow that post before running the example here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* Launch first seldon deployment with grpc or rest \n",
    "\n",
    "### Steps\n",
    "1. Define a seldon python component\n",
    "2. Build docker image\n",
    "3. Run a container based on docker image to test the endpoint\n",
    "4. Define SeldonDeployment yaml file \n",
    "5. `kubectl apply` SeldonDeployment to the kubernetes cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Python Component\n",
    "I'm taking this example code directly from [seldon-core irisClassifier example](https://github.com/SeldonIO/seldon-core/blob/master/examples/models/sklearn_iris/sklearn_iris.ipynb). \n",
    "First, we train a model based on the iris dataset included in the sklearn package, then we serve that trained model in the seldon endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: iris_classifier: File exists\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!mkdir iris_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/train_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/train_iris.py\n",
    "#collapse_show\n",
    "#hide_output\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "OUTPUT_FILE = \"iris_classifier/IrisClassifier.sav\"\n",
    "\n",
    "\n",
    "print(\"Loading iris data set...\")\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "p = Pipeline([(\"clf\", clf)])\n",
    "print(\"Training model...\")\n",
    "p.fit(X, y)\n",
    "print(\"Model trained!\")\n",
    "\n",
    "print(f\"Saving model in {OUTPUT_FILE}\")\n",
    "joblib.dump(p, OUTPUT_FILE)\n",
    "print(\"Model saved!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iris data set...\n",
      "Dataset loaded!\n",
      "Training model...\n",
      "Model trained!\n",
      "Saving model in iris_classifier/IrisClassifier.sav\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!python iris_classifier/train_iris.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the seldon python component that will be used to serve the model. \n",
    "Seldon has a few [components](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/python_component.html). In this example, we only use the Model component. Seldon components hold the logic that will be implanted into the serving endpoint that seldon creates. The model component must have a predict function, which is called when the future endpoint is hit. \n",
    "The reason seldon is so useful is because this is the only python code we need to write to serve this model. Seldon provides the rest of the logic, which puts this component into a web server, to serve the model.\n",
    "\n",
    "An important note about this section is that you'lll see the file is named `IrisClassifier.py`, which is camelcased. This is important, and you should not change this. The file name and the python component class name **must match**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/IrisClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/IrisClassifier.py\n",
    "#collapse_show\n",
    "#hide_output\n",
    "import joblib\n",
    "\n",
    "class IrisClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = joblib.load('IrisClassifier.sav')\n",
    "\n",
    "    def predict(self,X,features_names):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Image\n",
    "After defining a python component, there are two ways to create the docker image necessary for deployment. \n",
    "* [define a Dockerfile](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/python_wrapping_docker.html) which launches the seldon microservice\n",
    "* use [s2i](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/wrappers/s2i.html) to build the image directly from source code. \n",
    "\n",
    "I prefer manually defining a Dockerfile because it provides more control over the process. However, s2i is a great tool that works just as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write requirements.txt \n",
    "We must write a requirements.txt library with all requirements for the docker image listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/requirements.txt\n",
    "#hide_output\n",
    "sklearn\n",
    "seldon-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Dockerfile\n",
    "The Dockerfile follows the example provided [here](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/python_wrapping_docker.html). \n",
    "We start from the python:3.7-slim base image, copy the code from the current directory, which includes the python component we defined earlier, install requirements, then expose port 5000 for the microservice to run. \n",
    "Next, we define seldon specific variables. \n",
    "* MODEL_NAME must match the python file name (which also much match the python component class name). \n",
    "* API_TYPE can be either REST or GRPC.\n",
    "* SERVICE_TYPE is the type of seldon component. MODEL for this example. (explore the other seldon components [here]()\n",
    "* PERSISTENCE: 0 or 1. Defaults to 0. If it is set to 1, the component class will be periodically persisted to reis. This s unnecessary for our case because the component class will not change.\n",
    "    * this would be more pertinent for components like routers, which can have updating states for long running jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/Dockerfile\n",
    "#collapse_show\n",
    "#hide_output\n",
    "FROM python:3.7-slim\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "RUN pip install -r requirements.txt\n",
    "EXPOSE 5000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_NAME IrisClassifier \n",
    "ENV API_TYPE REST\n",
    "ENV SERVICE_TYPE MODEL \n",
    "ENV PERSISTENCE 0\n",
    "\n",
    "# seldon-core-microservice is a command line tool installed with the seldon-core python libray. You can use this locally as well!\n",
    "CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this example, let's build and run the docker image! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker Build\n",
    "Pass the iris_classifier dir where the image guts live, then pass a -t to tag the image with a name referring to your preferred docker image repository (I'm running on locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/10 : FROM python:3.7-slim\n",
      " ---> b386e7420fc3\n",
      "Step 2/10 : COPY . /app\n",
      " ---> 4ff1fc2d09e5\n",
      "Step 3/10 : WORKDIR /app\n",
      " ---> Running in 0e5b783b9df2\n",
      "Removing intermediate container 0e5b783b9df2\n",
      " ---> 840bd996fe26\n",
      "Step 4/10 : RUN pip install -r requirements.txt\n",
      " ---> Running in 6f1af0205271\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting seldon-core\n",
      "  Downloading seldon_core-1.2.2-py3-none-any.whl (108 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting Flask<2.0.0\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting Flask-cors<4.0.0\n",
      "  Downloading Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting prometheus-client<0.9.0,>=0.7.1\n",
      "  Downloading prometheus_client-0.8.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting Flask-OpenTracing<1.2.0,>=1.1.0\n",
      "  Downloading Flask-OpenTracing-1.1.0.tar.gz (8.2 kB)\n",
      "Collecting opentracing<2.4.0,>=2.2.0\n",
      "  Downloading opentracing-2.3.0.tar.gz (48 kB)\n",
      "Collecting requests<3.0.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting protobuf<4.0.0\n",
      "  Downloading protobuf-3.12.4-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting gunicorn<20.1.0,>=19.9.0\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "Collecting PyYAML<5.4\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting jsonschema<4.0.0\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting jaeger-client<4.4.0,>=4.1.0\n",
      "  Downloading jaeger-client-4.3.0.tar.gz (81 kB)\n",
      "Collecting numpy<2.0.0\n",
      "  Downloading numpy-1.19.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting grpcio<2.0.0\n",
      "  Downloading grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting flatbuffers<2.0.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting minio<6.0.0,>=4.0.9\n",
      "  Downloading minio-5.0.10-py2.py3-none-any.whl (75 kB)\n",
      "Collecting grpcio-opentracing<1.2.0,>=1.1.4\n",
      "  Downloading grpcio_opentracing-1.1.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from seldon-core->-r requirements.txt (line 3)) (47.3.1)\n",
      "Collecting redis<4.0.0\n",
      "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting Six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.16.0.tar.gz (108 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting threadloop<2,>=1\n",
      "  Downloading threadloop-1.0.2.tar.gz (4.9 kB)\n",
      "Collecting thrift\n",
      "  Downloading thrift-0.13.0.tar.gz (59 kB)\n",
      "Collecting tornado>=4.3\n",
      "  Downloading tornado-6.0.4.tar.gz (496 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting configparser\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: sklearn, Flask-OpenTracing, opentracing, PyYAML, jaeger-client, pyrsistent, threadloop, thrift, tornado\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=f35bba8878dbd98e914c5a5adaf27a0f1d876a5edecb8bc12a6fc820567cb767\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "  Building wheel for Flask-OpenTracing (setup.py): started\n",
      "  Building wheel for Flask-OpenTracing (setup.py): finished with status 'done'\n",
      "  Created wheel for Flask-OpenTracing: filename=Flask_OpenTracing-1.1.0-py3-none-any.whl size=9070 sha256=a5523f52ee6f8ac6d14676c4ae4366329ed5d0de51efad438d62490954d8b0a0\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/22/cd/ccb93fa68f4a01fb6c10082f97bcb2af9eb8e43565ce38a292\n",
      "  Building wheel for opentracing (setup.py): started\n",
      "  Building wheel for opentracing (setup.py): finished with status 'done'\n",
      "  Created wheel for opentracing: filename=opentracing-2.3.0-py3-none-any.whl size=51347 sha256=af662d9bfa99e590a3ea577797b851120b02e5aedd922a24daaa23c38c37d2c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/c5/4b/b030afc055aa78698cd96eb4b168b7f91bd9254191bf4e9f9f\n",
      "  Building wheel for PyYAML (setup.py): started\n",
      "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=13c5467ee2b39b34ccdcb5c19c4b242d07b2524ea74a5ffa8973edd82eb512c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "  Building wheel for jaeger-client (setup.py): started\n",
      "  Building wheel for jaeger-client (setup.py): finished with status 'done'\n",
      "  Created wheel for jaeger-client: filename=jaeger_client-4.3.0-py3-none-any.whl size=64291 sha256=10455b57709cf0e0639dbc033320426fe2f6b55bf3861f06bc4d6dd1d010bcf1\n",
      "  Stored in directory: /root/.cache/pip/wheels/4b/b9/d9/efe18893b02a4bc5abb68e0174d4ab10147f7f184dd170758e\n",
      "  Building wheel for pyrsistent (setup.py): started\n",
      "  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.16.0-cp37-cp37m-linux_x86_64.whl size=56582 sha256=e91c522596849dad9a07d485e0370d16ab53d3cbc0bf0a3119402ac2b5f5be5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/52/11/f0920f95c23ed7d2d0b05f2b7b2f4509e87a20cfe8ea43d987\n",
      "  Building wheel for threadloop (setup.py): started\n",
      "  Building wheel for threadloop (setup.py): finished with status 'done'\n",
      "  Created wheel for threadloop: filename=threadloop-1.0.2-py3-none-any.whl size=3423 sha256=fb54794ec4429b21ce2ec715b80c85e4c6acb18f43a5c09dad6de997ab9f90ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/93/e3/037c2555d98964d9ca537dabb39827a2b72470a679b5c0de37\n",
      "  Building wheel for thrift (setup.py): started\n",
      "  Building wheel for thrift (setup.py): finished with status 'done'\n",
      "  Created wheel for thrift: filename=thrift-0.13.0-py3-none-any.whl size=154885 sha256=286ebdc004bd104a372a362985f877758255719a3128b7b4c712a7ce3af932bb\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/35/5a/19f5dadf91f62bd783aaa8385f700de9bc14772e09ab0f006a\n",
      "  Building wheel for tornado (setup.py): started\n",
      "  Building wheel for tornado (setup.py): finished with status 'done'\n",
      "  Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=415150 sha256=87125dcb289d087fca2d318e4df34ee088cc7c5351a0f31862c1c9c8b171df2a\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348\n",
      "Successfully built sklearn Flask-OpenTracing opentracing PyYAML jaeger-client pyrsistent threadloop thrift tornado\n",
      "Installing collected packages: threadpoolctl, numpy, scipy, joblib, scikit-learn, sklearn, itsdangerous, Werkzeug, MarkupSafe, Jinja2, click, Flask, Six, Flask-cors, prometheus-client, opentracing, Flask-OpenTracing, urllib3, chardet, certifi, idna, requests, protobuf, gunicorn, PyYAML, pyrsistent, zipp, importlib-metadata, attrs, jsonschema, tornado, threadloop, thrift, jaeger-client, grpcio, flatbuffers, pytz, python-dateutil, configparser, minio, grpcio-opentracing, redis, seldon-core\n",
      "Successfully installed Flask-1.1.2 Flask-OpenTracing-1.1.0 Flask-cors-3.0.8 Jinja2-2.11.2 MarkupSafe-1.1.1 PyYAML-5.3.1 Six-1.15.0 Werkzeug-1.0.1 attrs-19.3.0 certifi-2020.6.20 chardet-3.0.4 click-7.1.2 configparser-5.0.0 flatbuffers-1.12 grpcio-1.30.0 grpcio-opentracing-1.1.4 gunicorn-20.0.4 idna-2.10 importlib-metadata-1.7.0 itsdangerous-1.1.0 jaeger-client-4.3.0 joblib-0.16.0 jsonschema-3.2.0 minio-5.0.10 numpy-1.19.1 opentracing-2.3.0 prometheus-client-0.8.0 protobuf-3.12.4 pyrsistent-0.16.0 python-dateutil-2.8.1 pytz-2020.1 redis-3.5.3 requests-2.24.0 scikit-learn-0.23.2 scipy-1.5.2 seldon-core-1.2.2 sklearn-0.0 threadloop-1.0.2 threadpoolctl-2.1.0 thrift-0.13.0 tornado-6.0.4 urllib3-1.25.10 zipp-3.1.0\n",
      "\u001b[91mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 6f1af0205271\n",
      " ---> 0dc34bb5fbcb\n",
      "Step 5/10 : EXPOSE 5000\n",
      " ---> Running in b5eb29d44f0a\n",
      "Removing intermediate container b5eb29d44f0a\n",
      " ---> 917767077f48\n",
      "Step 6/10 : ENV MODEL_NAME IrisClassifier\n",
      " ---> Running in e4774d29be83\n",
      "Removing intermediate container e4774d29be83\n",
      " ---> 3c496258db36\n",
      "Step 7/10 : ENV API_TYPE REST\n",
      " ---> Running in be32460a5412\n",
      "Removing intermediate container be32460a5412\n",
      " ---> 7f22bdd98919\n",
      "Step 8/10 : ENV SERVICE_TYPE MODEL\n",
      " ---> Running in d6b8303554c4\n",
      "Removing intermediate container d6b8303554c4\n",
      " ---> 90ba6accec5f\n",
      "Step 9/10 : ENV PERSISTENCE 0\n",
      " ---> Running in 5d7de4dc3ce0\n",
      "Removing intermediate container 5d7de4dc3ce0\n",
      " ---> f1865fdf1b32\n",
      "Step 10/10 : CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE\n",
      " ---> Running in 29a17b3da061\n",
      "Removing intermediate container 29a17b3da061\n",
      " ---> 33de3b5555b6\n",
      "Successfully built 33de3b5555b6\n",
      "Successfully tagged localhost:5000/iris_ex:latest\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!docker build iris_classifier/ -t localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Image\n",
    "You can test your newly created image by running the image and hitting the endpoint. \n",
    "You may ask yourself at this point, \"if I have a working docker image, what do I need kubernetes for?\" \n",
    "This is a great question. For simple use cases, this docker image itself is all you need, and you could run it as a standalone service. If the load is small and you can run it without any load balancing functionalities, you are good to go. \n",
    "However, kubernetes is a container orchestration engine. That means it is built to handle complex containerized applications and will make your life much easier if you need to handle more complex operations for applications that need to serve on a large scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d88f1163a71622fc2b67f33b8af4e95c2c8dafa9da43e2fe8c06e4322b7591c\n"
     ]
    }
   ],
   "source": [
    "!docker run --name \"iris_predictor\" -d --rm -p 5001:5000 localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also remove the -d argument from the above command and run this command in a separate window to see the log output while sending requests to the endpoint. Test the endpoint with the curl below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grpc \n",
    "from seldon_core.proto import prediction_pb2\n",
    "from seldon_core.proto import prediction_pb2_grpc\n",
    "\n",
    "\n",
    "### Test Rest Endpoint\n",
    "!curl -s http://localhost:5001/predict -H \"Content-Type: application/json\" -d '{\"data\":{\"ndarray\":[[5.964,4.006,2.081,1.031]]}}'\n",
    "\n",
    "\n",
    "### Test GRPC Endpoint\n",
    "# data = np.array([[5.964,4.006,2.081,1.031]])\n",
    "\n",
    "# datadef = prediction_pb2.DefaultData(\n",
    "#     tensor=prediction_pb2.Tensor(shape=data.shape, values=data.flatten())\n",
    "# )\n",
    "# request = prediction_pb2.SeldonMessage(data=datadef)\n",
    "# with grpc.insecure_channel(\"localhost:5001\") as channel:\n",
    "#     stub = prediction_pb2_grpc.ModelStub(channel)\n",
    "#     response = stub.Predict(request=request)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see successful output, you have your first seldon-core-microservice up and running! Now, we will deploy this as a simple inference graph on our kubernetes cluster. \n",
    "First, let's take down the running docker container:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, need to define our deployment configuration file. Here is a seldon config file for our deployment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_predictor\n"
     ]
    }
   ],
   "source": [
    "!docker container rm iris_predictor --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/sklearn_iris_deployment.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/sklearn_iris_deployment.yaml\n",
    "#hide_output\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-deployment-example\n",
    "spec:\n",
    "  name: sklearn-iris-deployment\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/sklearn-iris:0.1\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          name: sklearn-iris-classifier\n",
    "    graph:\n",
    "      children: []\n",
    "      endpoint:\n",
    "        type: REST\n",
    "      name: sklearn-iris-classifier\n",
    "      type: MODEL\n",
    "    name: sklearn-iris-predictor\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important notes about the deployment config: \n",
    "* apiVersion: this sends out request to the appropriate endpoint of the kubernets api, which was installed by helm earlier in this tutorial\n",
    "* kind: tells Kubernetes what kind of resource to create. \n",
    "* metadata: add labels, like name, to the deployment\n",
    "* spec: \n",
    "    * predictors: this is a list of predictors to deploy. It is a list because you have the option to create multiple inference graphs in the same spec. This is useful for things like Canary deployment, where you only want a new graph to recieve a small percentage of traffic\n",
    "        * componentSpecs: add information about the containers that need to be pulled to create our graph. In our case, we only need a single containe to serve our model. If we were creating a more complex inference graph (maybe with a transformer, router, and another model, then we would need to include the docker containers that house them in this section)\n",
    "        * graph: this is where you define the flow of components. This is easy in our case, there is only one component so we define one endpoint with no children. If there were more compnoents, we would fill out the children componenets in the children attriubte of the head of the graph. Seldon graphs are built implicitly through the use of the children attribute of each node in the graph. \n",
    "        \n",
    "There is one last step to deploy our graph, we must push our docker container to a registry! I am running a local registry with my kind cluster, thanks to the script given [here](https://kind.sigs.k8s.io/docs/user/local-registry/). You can also push to DockerHub as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [localhost:5000/iris_ex]\n",
      "\n",
      "\u001b[1B60d57b93: Preparing \n",
      "\u001b[1B43291ec5: Preparing \n",
      "\u001b[1B63f2d025: Preparing \n",
      "\u001b[1Bf01300cf: Preparing \n",
      "\u001b[1Ba0be9040: Preparing \n",
      "\u001b[1B1a837902: Preparing \n",
      "\u001b[7B60d57b93: Pushed     276MB/269.5MBA\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:b100e77cc2fc9b8b22f043f5d2c061f4d85bf891d84e21641c0cec1844f345f2 size: 1792\n"
     ]
    }
   ],
   "source": [
    "!docker push localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our docker image in a registry, it is available to our cluster, so we can deploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/seldon-deployment-example created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f iris_classifier/sklearn_iris_deployment.yaml\n",
    "from time import sleep\n",
    "sleep(5) # give the clsuter some to get the deployment running before executing the rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"seldon-92a927e5e90d7602e08ba9b9304f70e8\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"seldon-92a927e5e90d7602e08ba9b9304f70e8\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-deployment-example \\\n",
    "                                 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment is ready, you will need to port-forward the pod to your localhost in order check the request. That can be done wiht kubectl port-forward command \n",
    "```bash \n",
    "kubectl port-forward $(kubectl get pods -l seldon-app=seldon-deployment-example-sklearn-iris-predictor -o jsonpath='{.items[0].metadata.name}') 9000:9000\n",
    "```\n",
    "\n",
    "You must run this command in a separate window because it will need to run while we curl the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(prediction_pb2_grpc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"data\":{\"names\":[\"t:0\",\"t:1\",\"t:2\"],\"ndarray\":[[0.9548873249364169,0.04505474761561406,5.792744796895234e-05]]},\"meta\":{}}']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import grpc \n",
    "from seldon_core.proto import prediction_pb2\n",
    "from seldon_core.proto import prediction_pb2_grpc\n",
    "\n",
    "\n",
    "### Test REST endpoint\n",
    "res = !curl -s http://localhost:9000/predict -H \"Content-Type: application/json\" -d '{\"data\":{\"ndarray\":[[5.964,4.006,2.081,1.031]]}}'\n",
    "print(res)\n",
    "\n",
    "### Test GRPC endpoint\n",
    "# data = np.array([[5.964,4.006,2.081,1.031]])\n",
    "\n",
    "# datadef = prediction_pb2.DefaultData(tensor=prediction_pb2.Tensor(shape=data.shape, values=data.flatten()))\n",
    "# request = prediction_pb2.SeldonMessage(data=datadef)\n",
    "# with grpc.insecure_channel(\"localhost:9000/predict\") as channel:\n",
    "#     stub = prediction_pb2_grpc.ModelStub(channel)\n",
    "#     print(dir(stub))\n",
    "#     response = stub.Predict(request=request)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully created a seldon endpoint on kubernetes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"seldon-deployment-example\" deleted\n"
     ]
    }
   ],
   "source": [
    "## Cleanup\n",
    "!kubectl delete -f sklearn_iris_deployment.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "In this quick example, we scratched the surface of seldon-core by deploying a simple model endpoint on kubernetes. \n",
    "If you are hungry for more, chech out more of the posts in the [Seldon Super Series](). There, you can find notebooks similar to this that deploy more complex inference graphs, or dive into the underlying kubernetes concepts that seldon uses to make this possible! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Up\n",
    "* other seldon components \n",
    "* seldon graph construction \n",
    "* multi-component inference graph\n",
    "* operators and custom resources "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
