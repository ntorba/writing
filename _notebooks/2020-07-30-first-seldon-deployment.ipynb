{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Seldon Deployment\n",
    "> Get an ML endpoint up and running on your cluster!\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [kubernetes, docker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "In this post, we will walkthrough the first steps towards launching your own seldon deployment! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch cluster\n",
    "To get started, let's get a cluster up and running. If you followed [part 1]() of this series, you can do so with kind. \n",
    "Below, I create a cluster, create a namespace called seldon-intro, then use `kubens` to make seldon-into my default namespace (so I don't have to include it in every command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"kind\" ...\n",
      " \u001b[32mâœ“\u001b[0m Ensuring node image (kindest/node:v1.17.0) ðŸ–¼\n",
      " \u001b[32mâœ“\u001b[0m Preparing nodes ðŸ“¦ 7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\n",
      " \u001b[32mâœ“\u001b[0m Writing configuration ðŸ“œ7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\n",
      " \u001b[32mâœ“\u001b[0m Starting control-plane ðŸ•¹ï¸7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\n",
      " \u001b[32mâœ“\u001b[0m Installing CNI ðŸ”Œ7l\u001b[?7l\u001b[?7l\u001b[?7l\n",
      " \u001b[32mâœ“\u001b[0m Installing StorageClass ðŸ’¾7l\u001b[?7l\u001b[?7l\u001b[?7l\u001b[?7l\n",
      "Set kubectl context to \"kind-kind\"\n",
      "You can now use your cluster with:\n",
      "\n",
      "kubectl cluster-info --context kind-kind\n",
      "\n",
      "Not sure what to do next? ðŸ˜… Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n",
      "namespace/seldon-intro created\n",
      "Context \"kind-kind\" modified.\n",
      "Active namespace is \"seldon-intro\".\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster \n",
    "!kubectl create namespace seldon-intro\n",
    "!kubens seldon-intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install seldon-core \n",
    "To install seldon-core on the cluster, use helm. To install helm itself, find directions [here](https://helm.sh/), or use `brew install helm` on mac.\n",
    "\n",
    "Once helm is installed, use it to install seldon-core and seldon-core-operator with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: seldon-core\n",
      "LAST DEPLOYED: Fri Jul 31 07:09:00 2020\n",
      "NAMESPACE: seldon-intro\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core seldon-core-operator \\\n",
    "    --repo https://storage.googleapis.com/seldon-charts \\\n",
    "    --set usageMetrics.enabled=true \\\n",
    "    --set ambassador.enabled=true \\\n",
    "    --namespace seldon-intro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the install is running correctly, run the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "seldon-controller-manager-bc4b8bf64-fssv5   1/1     Running   0          37s\n",
      "-----\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "seldon-controller-manager   1/1     1            1           45s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods\n",
    "print(\"-----\")\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a pod and deployment with `seldon-controller-manager` in the name. This pod and deployment house the seldon-core operator, which is an extensions to to the kubernetes api that allows us to deploy seldon inference graphs later on. We don't need to worry much about these details for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build example\n",
    "I'm taking this example code directly from [seldon-core irisClassifier example](https://github.com/SeldonIO/seldon-core/blob/master/examples/models/sklearn_iris/sklearn_iris.ipynb). \n",
    "This is a classic sklearn example we will be able to get up quick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: iris_classifier: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir iris_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/train_iris.py\n"
     ]
    }
   ],
   "source": [
    "#collapse_show\n",
    "%%writefile iris_classifier/train_iris.py\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "OUTPUT_FILE = \"iris_classifier/IrisClassifier.sav\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    clf = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    p = Pipeline([(\"clf\", clf)])\n",
    "    print(\"Training model...\")\n",
    "    p.fit(X, y)\n",
    "    print(\"Model trained!\")\n",
    "\n",
    "    print(f\"Saving model in {OUTPUT_FILE}\")\n",
    "    joblib.dump(p, OUTPUT_FILE)\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading iris data set...\")\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    print(\"Dataset loaded!\")\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iris data set...\n",
      "Dataset loaded!\n",
      "Training model...\n",
      "Model trained!\n",
      "Saving model in iris_classifier/IrisClassifier.sav\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "!python iris_classifier/train_iris.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/IrisClassifier.py\n"
     ]
    }
   ],
   "source": [
    "#collapse_show\n",
    "%%writefile iris_classifier/IrisClassifier.py\n",
    "import joblib\n",
    "\n",
    "class IrisClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = joblib.load('IrisClassifier.sav')\n",
    "\n",
    "    def predict(self,X,features_names):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to slightly differ from their example, and use a Dockerfile to create the docker image for this component instead of s2i. Feel free to use s2i directly from their example instead! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/requirements.txt\n",
    "sklearn\n",
    "seldon-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "#collapse_show\n",
    "%%writefile iris_classifier/Dockerfile\n",
    "FROM python:3.7-slim\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "RUN pip install -r requirements.txt\n",
    "EXPOSE 5000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_NAME IrisClassifier \n",
    "ENV API_TYPE REST\n",
    "ENV SERVICE_TYPE MODEL \n",
    "ENV PERSISTENCE 0\n",
    "\n",
    "# seldon-core-microservice is a command line tool installed with the seldon-core python libray. You can use this locally as well!\n",
    "CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this example, let's build and run the docker image! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.26kB\n",
      "Step 1/10 : FROM python:3.7-slim\n",
      " ---> b386e7420fc3\n",
      "Step 2/10 : COPY . /app\n",
      " ---> Using cache\n",
      " ---> 350aae576a5c\n",
      "Step 3/10 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 752b217417eb\n",
      "Step 4/10 : RUN pip install -r requirements.txt\n",
      " ---> Running in 986fd56d5823\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting seldon-core\n",
      "  Downloading seldon_core-1.2.2-py3-none-any.whl (108 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting Flask-cors<4.0.0\n",
      "  Downloading Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting numpy<2.0.0\n",
      "  Downloading numpy-1.19.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting prometheus-client<0.9.0,>=0.7.1\n",
      "  Downloading prometheus_client-0.8.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting Flask<2.0.0\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting jaeger-client<4.4.0,>=4.1.0\n",
      "  Downloading jaeger-client-4.3.0.tar.gz (81 kB)\n",
      "Collecting grpcio<2.0.0\n",
      "  Downloading grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting Flask-OpenTracing<1.2.0,>=1.1.0\n",
      "  Downloading Flask-OpenTracing-1.1.0.tar.gz (8.2 kB)\n",
      "Collecting requests<3.0.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting PyYAML<5.4\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting jsonschema<4.0.0\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting flatbuffers<2.0.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting grpcio-opentracing<1.2.0,>=1.1.4\n",
      "  Downloading grpcio_opentracing-1.1.4-py3-none-any.whl (14 kB)\n",
      "Collecting gunicorn<20.1.0,>=19.9.0\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "Collecting minio<6.0.0,>=4.0.9\n",
      "  Downloading minio-5.0.10-py2.py3-none-any.whl (75 kB)\n",
      "Collecting redis<4.0.0\n",
      "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "Collecting opentracing<2.4.0,>=2.2.0\n",
      "  Downloading opentracing-2.3.0.tar.gz (48 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from seldon-core->-r requirements.txt (line 2)) (47.3.1)\n",
      "Collecting protobuf<4.0.0\n",
      "  Downloading protobuf-3.12.4-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Collecting Six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting threadloop<2,>=1\n",
      "  Downloading threadloop-1.0.2.tar.gz (4.9 kB)\n",
      "Collecting thrift\n",
      "  Downloading thrift-0.13.0.tar.gz (59 kB)\n",
      "Collecting tornado>=4.3\n",
      "  Downloading tornado-6.0.4.tar.gz (496 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.16.0.tar.gz (108 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting configparser\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: sklearn, jaeger-client, Flask-OpenTracing, PyYAML, opentracing, threadloop, thrift, tornado, pyrsistent\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=16fbb8bcbb99def9b515836945c545b5713029e56ae063a6536c1fecdc81cd2e\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "  Building wheel for jaeger-client (setup.py): started\n",
      "  Building wheel for jaeger-client (setup.py): finished with status 'done'\n",
      "  Created wheel for jaeger-client: filename=jaeger_client-4.3.0-py3-none-any.whl size=64291 sha256=30e266bd95e0e856bc222089f5d260e2abbe0df294ad95181279d66bcf158641\n",
      "  Stored in directory: /root/.cache/pip/wheels/4b/b9/d9/efe18893b02a4bc5abb68e0174d4ab10147f7f184dd170758e\n",
      "  Building wheel for Flask-OpenTracing (setup.py): started\n",
      "  Building wheel for Flask-OpenTracing (setup.py): finished with status 'done'\n",
      "  Created wheel for Flask-OpenTracing: filename=Flask_OpenTracing-1.1.0-py3-none-any.whl size=9070 sha256=ec15ccf40ef050f7a3b21bc296dc7521b80d5450653f6f6f9059ea8cd56dc6a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/22/cd/ccb93fa68f4a01fb6c10082f97bcb2af9eb8e43565ce38a292\n",
      "  Building wheel for PyYAML (setup.py): started\n",
      "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=a11f68ce674a39d2cd3d5602023174b811d0a095e5b377c3243f5a73324fb02b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "  Building wheel for opentracing (setup.py): started\n",
      "  Building wheel for opentracing (setup.py): finished with status 'done'\n",
      "  Created wheel for opentracing: filename=opentracing-2.3.0-py3-none-any.whl size=51347 sha256=690653e80c043efdd79dde72655e347329960b97ea8353859d9ae5d97a912a3a\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/c5/4b/b030afc055aa78698cd96eb4b168b7f91bd9254191bf4e9f9f\n",
      "  Building wheel for threadloop (setup.py): started\n",
      "  Building wheel for threadloop (setup.py): finished with status 'done'\n",
      "  Created wheel for threadloop: filename=threadloop-1.0.2-py3-none-any.whl size=3423 sha256=06a147348e299106c61d1897358e1fce2ac763aabe8e45a8aab84effdd97b84e\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/93/e3/037c2555d98964d9ca537dabb39827a2b72470a679b5c0de37\n",
      "  Building wheel for thrift (setup.py): started\n",
      "  Building wheel for thrift (setup.py): finished with status 'done'\n",
      "  Created wheel for thrift: filename=thrift-0.13.0-py3-none-any.whl size=154885 sha256=26de2f070f6bf7f8e615a995f161662f2f407c7824c0a997580f12285dac4ef7\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/35/5a/19f5dadf91f62bd783aaa8385f700de9bc14772e09ab0f006a\n",
      "  Building wheel for tornado (setup.py): started\n",
      "  Building wheel for tornado (setup.py): finished with status 'done'\n",
      "  Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=415150 sha256=445e7e4c2816d9f0c6e051164800f343d718866eee08381c936e33b8c30f1e83\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348\n",
      "  Building wheel for pyrsistent (setup.py): started\n",
      "  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.16.0-cp37-cp37m-linux_x86_64.whl size=56582 sha256=fb2ca71f57ff9098bdc84270d35f834ed2093a0ab42b959c74c6fe52dc99f02f\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/52/11/f0920f95c23ed7d2d0b05f2b7b2f4509e87a20cfe8ea43d987\n",
      "Successfully built sklearn jaeger-client Flask-OpenTracing PyYAML opentracing threadloop thrift tornado pyrsistent\n",
      "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn, sklearn, Six, Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, Flask, Flask-cors, prometheus-client, tornado, threadloop, thrift, opentracing, jaeger-client, grpcio, Flask-OpenTracing, idna, certifi, urllib3, chardet, requests, PyYAML, pyrsistent, attrs, zipp, importlib-metadata, jsonschema, flatbuffers, grpcio-opentracing, gunicorn, configparser, pytz, python-dateutil, minio, redis, protobuf, seldon-core\n",
      "Successfully installed Flask-1.1.2 Flask-OpenTracing-1.1.0 Flask-cors-3.0.8 Jinja2-2.11.2 MarkupSafe-1.1.1 PyYAML-5.3.1 Six-1.15.0 Werkzeug-1.0.1 attrs-19.3.0 certifi-2020.6.20 chardet-3.0.4 click-7.1.2 configparser-5.0.0 flatbuffers-1.12 grpcio-1.30.0 grpcio-opentracing-1.1.4 gunicorn-20.0.4 idna-2.10 importlib-metadata-1.7.0 itsdangerous-1.1.0 jaeger-client-4.3.0 joblib-0.16.0 jsonschema-3.2.0 minio-5.0.10 numpy-1.19.1 opentracing-2.3.0 prometheus-client-0.8.0 protobuf-3.12.4 pyrsistent-0.16.0 python-dateutil-2.8.1 pytz-2020.1 redis-3.5.3 requests-2.24.0 scikit-learn-0.23.1 scipy-1.5.2 seldon-core-1.2.2 sklearn-0.0 threadloop-1.0.2 threadpoolctl-2.1.0 thrift-0.13.0 tornado-6.0.4 urllib3-1.25.10 zipp-3.1.0\n",
      "\u001b[91mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 986fd56d5823\n",
      " ---> 2f542cde62d4\n",
      "Step 5/10 : EXPOSE 5000\n",
      " ---> Running in 24901c6997b1\n",
      "Removing intermediate container 24901c6997b1\n",
      " ---> 6782d741c3d2\n",
      "Step 6/10 : ENV MODEL_NAME IrisClassifier\n",
      " ---> Running in 9b698bfac7f4\n",
      "Removing intermediate container 9b698bfac7f4\n",
      " ---> c3bd70bc0fe1\n",
      "Step 7/10 : ENV API_TYPE REST\n",
      " ---> Running in f689bffcbd76\n",
      "Removing intermediate container f689bffcbd76\n",
      " ---> 52c5cc72ae7a\n",
      "Step 8/10 : ENV SERVICE_TYPE MODEL\n",
      " ---> Running in 6d5298e74ca5\n",
      "Removing intermediate container 6d5298e74ca5\n",
      " ---> 17f1b34dc3c8\n",
      "Step 9/10 : ENV PERSISTENCE 0\n",
      " ---> Running in e8c85193be01\n",
      "Removing intermediate container e8c85193be01\n",
      " ---> bd8e46832189\n",
      "Step 10/10 : CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE\n",
      " ---> Running in 35a85afcc183\n",
      "Removing intermediate container 35a85afcc183\n",
      " ---> 83c4e3036e27\n",
      "Successfully built 83c4e3036e27\n",
      "Successfully tagged localhost:5000/iris_ex:latest\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "!docker build iris_classifier/ -t localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8cf5c32413068cdec2c01bd6b37d74f7401323ddeebc65c9e33e11b531c0cf0b\n"
     ]
    }
   ],
   "source": [
    "!docker run --name \"iris_predictor\" -d --rm -p 5001:5000 localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also remove the -d argument from the above command and run this command in a separate window to see the log output while sending requests to the endpoint. Test the endpoint with the curl below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[\"t:0\",\"t:1\",\"t:2\"],\"ndarray\":[[0.9548873249364059,0.04505474761562512,5.792744796895372e-05]]},\"meta\":{}}\n"
     ]
    }
   ],
   "source": [
    "!curl  -s http://localhost:5001/predict -H \"Content-Type: application/json\" -d '{\"data\":{\"ndarray\":[[5.964,4.006,2.081,1.031]]}}'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see successful output, you have your first seldon-core-microservice up and running! Now, we will deploy this as a simple inference graph on our kubernetes cluster. \n",
    "First, let's take down the running docker container:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, need to define our deployment configuration file. Here is a seldon config file for our deployment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_predictor\n"
     ]
    }
   ],
   "source": [
    "!docker container rm iris_predictor --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing iris_classifier/sklearn_iris_deployment.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier/sklearn_iris_deployment.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-deployment-example\n",
    "spec:\n",
    "  name: sklearn-iris-deployment\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/sklearn-iris:0.1\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          name: sklearn-iris-classifier\n",
    "    graph:\n",
    "      children: []\n",
    "      endpoint:\n",
    "        type: REST\n",
    "      name: sklearn-iris-classifier\n",
    "      type: MODEL\n",
    "    name: sklearn-iris-predictor\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important notes about the deployment config: \n",
    "* apiVersion: this sends out request to the appropriate endpoint of the kubernets api, which was installed by helm earlier in this tutorial\n",
    "* kind: tells Kubernetes what kind of resource to create. \n",
    "* metadata: add labels, like name, to the deployment\n",
    "* spec: \n",
    "    * predictors: this is a list of predictors to deploy. It is a list because you have the option to create multiple inference graphs in the same spec. This is useful for things like Canary deployment, where you only want a new graph to recieve a small percentage of traffic\n",
    "        * componentSpecs: add information about the containers that need to be pulled to create our graph. In our case, we only need a single containe to serve our model. If we were creating a more complex inference graph (maybe with a transformer, router, and another model, then we would need to include the docker containers that house them in this section)\n",
    "        * graph: this is where you define the flow of components. This is easy in our case, there is only one component so we define one endpoint with no children. If there were more compnoents, we would fill out the children componenets in the children attriubte of the head of the graph. Seldon graphs are built implicitly through the use of the children attribute of each node in the graph. \n",
    "        \n",
    "There is one last step to deploy our graph, we must push our docker container to a registry! I am running a local registry with my kind cluster, thanks to the script given [here](https://kind.sigs.k8s.io/docs/user/local-registry/). You can also push to DockerHub as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [localhost:5000/iris_ex]\n",
      "\n",
      "\u001b[1Bfd587320: Preparing \n",
      "\u001b[1B308ed686: Preparing \n",
      "\u001b[1B63f2d025: Preparing \n",
      "\u001b[1Bf01300cf: Preparing \n",
      "\u001b[1Ba0be9040: Preparing \n",
      "\u001b[1B1a837902: Preparing \n",
      "\u001b[7Bfd587320: Pushed     276MB/269.6MB\u001b[2A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2KPushing  109.8MB/269.6MB\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2KPushing    187MB/269.6MB\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:696a49e226fd1e9125a1af606abeae22b45d7770e0feea91600ba1ff0ddab50d size: 1792\n"
     ]
    }
   ],
   "source": [
    "!docker push localhost:5000/iris_ex:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our docker image in a registry, it is available to our cluster, so we can deploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/seldon-deployment-example created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f iris_classifier/sklearn_iris_deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"seldon-92a927e5e90d7602e08ba9b9304f70e8\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"seldon-92a927e5e90d7602e08ba9b9304f70e8\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-deployment-example \\\n",
    "                                 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment is ready, you will need to port-forward the pod to your localhost in order check the request. That can be done wiht kubectl port-forward command \n",
    "```bash \n",
    "kubectl port-forward $(kubectl get pods -l seldon-app=seldon-deployment-example-sklearn-iris-predictor -o jsonpath='{.items[0].metadata.na}') 9000:9000`\n",
    "```\n",
    "\n",
    "You must run this command in a separate window because it will need to run while we curl the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[\"t:0\",\"t:1\",\"t:2\"],\"ndarray\":[[0.9548873249364169,0.04505474761561406,5.792744796895234e-05]]},\"meta\":{}}\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:9000/predict -H \"Content-Type: application/json\" -d '{\"data\":{\"ndarray\":[[5.964,4.006,2.081,1.031]]}}'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully created a seldon endpoint on kubernetes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"seldon-deployment-example\" deleted\n"
     ]
    }
   ],
   "source": [
    "## Cleanup\n",
    "!kubectl delete -f sklearn_iris_deployment.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "In this quick example, we scratched the surface of seldon-core by deploying a simple model endpoint on kubernetes. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
