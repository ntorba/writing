{
  
    
        "post0": {
            "title": "Launch a Seldon Deployment",
            "content": "Intro . In this post, we will walkthrough the first steps towards launching your own seldon deployment! . Launch cluster . To get started, let&#39;s get a cluster up and running. If you followed part 1 of this series, you can do so with kind. Below, I create a cluster, create a namespace called seldon-intro, then use kubens to make seldon-into my default namespace (so I don&#39;t have to include it in every command). . !kind create cluster !kubectl create namespace seldon-intro !kubens seldon-intro . Install seldon-core . To install seldon-core on the cluster, use helm. To install helm itself, find directions here, or use brew install helm on mac. . Once helm is installed, use it to install seldon-core and seldon-core-operator with the following command: . !helm install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --set usageMetrics.enabled=true --set ambassador.enabled=true --namespace seldon-intro . To check the install is running correctly, run the following command: . !kubectl get deployments, pods . You should see a pod and deployment with seldon-controller-manager in the name. This pod and deployment house the seldon-core operator, which is an extensions to to the kubernetes api that allows us to deploy seldon inference graphs later on. We don&#39;t need to worry much about these details for now. . Build example . I&#39;m taking this example code directly from seldon-core irisClassifier example. This is a classic sklearn example we will be able to get up quick. . !mkdir iris_classifier . mkdir: iris_classifier: File exists . %%writefile iris_classifier/train_iris.py import joblib from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn import datasets OUTPUT_FILE = &quot;iris_classifier/IrisClassifier.sav&quot; def main(): clf = LogisticRegression(solver=&quot;liblinear&quot;, multi_class=&quot;ovr&quot;) p = Pipeline([(&quot;clf&quot;, clf)]) print(&quot;Training model...&quot;) p.fit(X, y) print(&quot;Model trained!&quot;) print(f&quot;Saving model in {OUTPUT_FILE}&quot;) joblib.dump(p, OUTPUT_FILE) print(&quot;Model saved!&quot;) if __name__ == &quot;__main__&quot;: print(&quot;Loading iris data set...&quot;) iris = datasets.load_iris() X, y = iris.data, iris.target print(&quot;Dataset loaded!&quot;) main() . Writing iris_classifier/train_iris.py . !python iris_classifier/train_iris.py . Loading iris data set... Dataset loaded! Training model... Model trained! Saving model in IrisClassifier.sav Model saved! . %%writefile iris_classifier/IrisClassifier.py import joblib class IrisClassifier(object): def __init__(self): self.model = joblib.load(&#39;IrisClassifier.sav&#39;) def predict(self,X,features_names): return self.model.predict_proba(X) . Writing iris_classifier/IrisClassifier.py . I&#39;m going to slightly differ from their example, and use a Dockerfile to create the docker image for this component instead of s2i. Feel free to use s2i directly from their example instead! . %%writefile iris_classifier/requirements.txt sklearn seldon-core . Writing iris_classifier/requirements.txt . %%writefile iris_classifier/Dockerfile FROM python:3.7-slim COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 5000 # Define environment variable ENV MODEL_NAME IrisClassifier ENV API_TYPE REST ENV SERVICE_TYPE MODEL ENV PERSISTENCE 0 # seldon-core-microservice is a command line tool installed with the seldon-core python libray. You can use this locally as well! CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE . Overwriting iris_classifier/Dockerfile . To test this example, let&#39;s build and run the docker image! . !docker build iris_classifier/ -t localhost:5000/iris_ex:latest . Sending build context to Docker daemon 7.168kB Step 1/10 : FROM python:3.7-slim &gt; b386e7420fc3 Step 2/10 : COPY . /app &gt; e89dd689e61c Step 3/10 : WORKDIR /app &gt; Running in 293fb2fc0432 Removing intermediate container 293fb2fc0432 &gt; 1815d73b62c4 Step 4/10 : RUN pip install -r requirements.txt &gt; Running in 611c0926cddc Collecting sklearn Downloading sklearn-0.0.tar.gz (1.1 kB) Collecting seldon-core Downloading seldon_core-1.2.2-py3-none-any.whl (108 kB) Collecting scikit-learn Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB) Collecting gunicorn&lt;20.1.0,&gt;=19.9.0 Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB) Collecting jaeger-client&lt;4.4.0,&gt;=4.1.0 Downloading jaeger-client-4.3.0.tar.gz (81 kB) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/site-packages (from seldon-core-&gt;-r requirements.txt (line 2)) (47.3.1) Collecting numpy&lt;2.0.0 Downloading numpy-1.19.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB) Collecting minio&lt;6.0.0,&gt;=4.0.9 Downloading minio-5.0.10-py2.py3-none-any.whl (75 kB) Collecting prometheus-client&lt;0.9.0,&gt;=0.7.1 Downloading prometheus_client-0.8.0-py2.py3-none-any.whl (53 kB) Collecting requests&lt;3.0.0 Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB) Collecting jsonschema&lt;4.0.0 Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB) Collecting Flask&lt;2.0.0 Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB) Collecting redis&lt;4.0.0 Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB) Collecting Flask-cors&lt;4.0.0 Downloading Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB) Collecting protobuf&lt;4.0.0 Downloading protobuf-3.12.4-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB) Collecting opentracing&lt;2.4.0,&gt;=2.2.0 Downloading opentracing-2.3.0.tar.gz (48 kB) Collecting Flask-OpenTracing&lt;1.2.0,&gt;=1.1.0 Downloading Flask-OpenTracing-1.1.0.tar.gz (8.2 kB) Collecting grpcio&lt;2.0.0 Downloading grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB) Collecting flatbuffers&lt;2.0.0 Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB) Collecting PyYAML&lt;5.4 Downloading PyYAML-5.3.1.tar.gz (269 kB) Collecting grpcio-opentracing&lt;1.2.0,&gt;=1.1.4 Downloading grpcio_opentracing-1.1.4-py3-none-any.whl (14 kB) Collecting scipy&gt;=0.19.1 Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) Collecting joblib&gt;=0.11 Downloading joblib-0.16.0-py3-none-any.whl (300 kB) Collecting threadpoolctl&gt;=2.0.0 Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB) Collecting threadloop&lt;2,&gt;=1 Downloading threadloop-1.0.2.tar.gz (4.9 kB) Collecting thrift Downloading thrift-0.13.0.tar.gz (59 kB) Collecting tornado&gt;=4.3 Downloading tornado-6.0.4.tar.gz (496 kB) Collecting certifi Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB) Collecting urllib3 Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB) Collecting pytz Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB) Collecting configparser Downloading configparser-5.0.0-py3-none-any.whl (22 kB) Collecting python-dateutil Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) Collecting idna&lt;3,&gt;=2.5 Downloading idna-2.10-py2.py3-none-any.whl (58 kB) Collecting chardet&lt;4,&gt;=3.0.2 Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB) Collecting attrs&gt;=17.4.0 Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB) Collecting six&gt;=1.11.0 Downloading six-1.15.0-py2.py3-none-any.whl (10 kB) Collecting pyrsistent&gt;=0.14.0 Downloading pyrsistent-0.16.0.tar.gz (108 kB) Collecting importlib-metadata; python_version &lt; &#34;3.8&#34; Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB) Collecting itsdangerous&gt;=0.24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB) Collecting Jinja2&gt;=2.10.1 Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB) Collecting click&gt;=5.1 Downloading click-7.1.2-py2.py3-none-any.whl (82 kB) Collecting Werkzeug&gt;=0.15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB) Collecting zipp&gt;=0.5 Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB) Collecting MarkupSafe&gt;=0.23 Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB) Building wheels for collected packages: sklearn, jaeger-client, opentracing, Flask-OpenTracing, PyYAML, threadloop, thrift, tornado, pyrsistent Building wheel for sklearn (setup.py): started Building wheel for sklearn (setup.py): finished with status &#39;done&#39; Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=c6663f95549bdd109fa052add555e0c4c160b52c7e008d8fea26d816f4db9053 Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e Building wheel for jaeger-client (setup.py): started Building wheel for jaeger-client (setup.py): finished with status &#39;done&#39; Created wheel for jaeger-client: filename=jaeger_client-4.3.0-py3-none-any.whl size=64291 sha256=0f18aca992ee72e11b05c7de433a8b5c5959dbb9240e6c1c7e322ee675959048 Stored in directory: /root/.cache/pip/wheels/4b/b9/d9/efe18893b02a4bc5abb68e0174d4ab10147f7f184dd170758e Building wheel for opentracing (setup.py): started Building wheel for opentracing (setup.py): finished with status &#39;done&#39; Created wheel for opentracing: filename=opentracing-2.3.0-py3-none-any.whl size=51347 sha256=f04a64db0214646796db1d7e861e1a1d398234fb60d2d807cf518b3a871e893e Stored in directory: /root/.cache/pip/wheels/19/c5/4b/b030afc055aa78698cd96eb4b168b7f91bd9254191bf4e9f9f Building wheel for Flask-OpenTracing (setup.py): started Building wheel for Flask-OpenTracing (setup.py): finished with status &#39;done&#39; Created wheel for Flask-OpenTracing: filename=Flask_OpenTracing-1.1.0-py3-none-any.whl size=9070 sha256=8889a367cf5e75a7e4416edead3de69fefe432d42b1d2e19c12eb2212597a34a Stored in directory: /root/.cache/pip/wheels/42/22/cd/ccb93fa68f4a01fb6c10082f97bcb2af9eb8e43565ce38a292 Building wheel for PyYAML (setup.py): started Building wheel for PyYAML (setup.py): finished with status &#39;done&#39; Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=8abe3f338d2c78bc974f7c9ae3440f99e6df55ee3b262ee6e663cdb6d8ba714e Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653 Building wheel for threadloop (setup.py): started Building wheel for threadloop (setup.py): finished with status &#39;done&#39; Created wheel for threadloop: filename=threadloop-1.0.2-py3-none-any.whl size=3423 sha256=18d6bf81e964b05bdda6d3b13e7216b6142041d2a699394a49fead7d6c0bdf4f Stored in directory: /root/.cache/pip/wheels/08/93/e3/037c2555d98964d9ca537dabb39827a2b72470a679b5c0de37 Building wheel for thrift (setup.py): started Building wheel for thrift (setup.py): finished with status &#39;done&#39; Created wheel for thrift: filename=thrift-0.13.0-py3-none-any.whl size=154885 sha256=502b079d519542e741efa3ca9dd5d3b5ef4f5b92c49756851c508dcb66a8169a Stored in directory: /root/.cache/pip/wheels/79/35/5a/19f5dadf91f62bd783aaa8385f700de9bc14772e09ab0f006a Building wheel for tornado (setup.py): started Building wheel for tornado (setup.py): finished with status &#39;done&#39; Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=415150 sha256=6fe87d010077fb2bda88f465f8284d05bb7f2e2d431f1ae47162a13026732ba1 Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348 Building wheel for pyrsistent (setup.py): started Building wheel for pyrsistent (setup.py): finished with status &#39;done&#39; Created wheel for pyrsistent: filename=pyrsistent-0.16.0-cp37-cp37m-linux_x86_64.whl size=56582 sha256=0ac45ee5becbce6f6f04ed951c2c44cb82239326c858ad46b9c50da04106c549 Stored in directory: /root/.cache/pip/wheels/22/52/11/f0920f95c23ed7d2d0b05f2b7b2f4509e87a20cfe8ea43d987 Successfully built sklearn jaeger-client opentracing Flask-OpenTracing PyYAML threadloop thrift tornado pyrsistent Installing collected packages: numpy, scipy, joblib, threadpoolctl, scikit-learn, sklearn, gunicorn, tornado, threadloop, six, thrift, opentracing, jaeger-client, certifi, urllib3, pytz, configparser, python-dateutil, minio, prometheus-client, idna, chardet, requests, attrs, pyrsistent, zipp, importlib-metadata, jsonschema, itsdangerous, MarkupSafe, Jinja2, click, Werkzeug, Flask, redis, Flask-cors, protobuf, Flask-OpenTracing, grpcio, flatbuffers, PyYAML, grpcio-opentracing, seldon-core Successfully installed Flask-1.1.2 Flask-OpenTracing-1.1.0 Flask-cors-3.0.8 Jinja2-2.11.2 MarkupSafe-1.1.1 PyYAML-5.3.1 Werkzeug-1.0.1 attrs-19.3.0 certifi-2020.6.20 chardet-3.0.4 click-7.1.2 configparser-5.0.0 flatbuffers-1.12 grpcio-1.30.0 grpcio-opentracing-1.1.4 gunicorn-20.0.4 idna-2.10 importlib-metadata-1.7.0 itsdangerous-1.1.0 jaeger-client-4.3.0 joblib-0.16.0 jsonschema-3.2.0 minio-5.0.10 numpy-1.19.1 opentracing-2.3.0 prometheus-client-0.8.0 protobuf-3.12.4 pyrsistent-0.16.0 python-dateutil-2.8.1 pytz-2020.1 redis-3.5.3 requests-2.24.0 scikit-learn-0.23.1 scipy-1.5.2 seldon-core-1.2.2 six-1.15.0 sklearn-0.0 threadloop-1.0.2 threadpoolctl-2.1.0 thrift-0.13.0 tornado-6.0.4 urllib3-1.25.10 zipp-3.1.0 WARNING: You are using pip version 20.1.1; however, version 20.2 is available. You should consider upgrading via the &#39;/usr/local/bin/python -m pip install --upgrade pip&#39; command. Removing intermediate container 611c0926cddc &gt; e5c9c25eab23 Step 5/10 : EXPOSE 5000 &gt; Running in b1becc35f355 Removing intermediate container b1becc35f355 &gt; 00cd89defe7e Step 6/10 : ENV MODEL_NAME IrisClassifer &gt; Running in 62c1a0b246f6 Removing intermediate container 62c1a0b246f6 &gt; 17eeec5d0d61 Step 7/10 : ENV API_TYPE REST &gt; Running in c03ff1837bd4 Removing intermediate container c03ff1837bd4 &gt; 6aaecc5b27bb Step 8/10 : ENV SERVICE_TYPE MODEL &gt; Running in 810e13bc1727 Removing intermediate container 810e13bc1727 &gt; b28972f57a5e Step 9/10 : ENV PERSISTENCE 0 &gt; Running in 162743c6449e Removing intermediate container 162743c6449e &gt; eb4e7d50cab1 Step 10/10 : CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE &gt; Running in 2b747f223cad Removing intermediate container 2b747f223cad &gt; 04094c475db6 Successfully built 04094c475db6 Successfully tagged localhost:5000/iris_ex:latest . !docker push localhost:5000/iris_ex:latest . The push refers to repository [localhost:5000/iris_ex] c29a6535: Preparing ba1ff41c: Preparing 63f2d025: Preparing f01300cf: Preparing a0be9040: Preparing 1a837902: Preparing c29a6535: Pushed 276MB/269.6MBousingpriceinterface_seldonoutputtransformer_reporter Pushing 231.5MB/269.6MBlatest: digest: sha256:218dbbe53e08de366e7e82147853be659f6f5f1c7c807f66c161fdb86fbd0d1c size: 1791 . !docker run --name &quot;iris_predictor1&quot; -d --rm -p 5001:5000 localhost:5000/iris_ex:latest . 44e899193312740cd72eab8a23d5dffdafb1f3725682710880dcaca5545b7386 . !curl -s http://localhost:5001/predict -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;data&quot;:{&quot;ndarray&quot;:[[5.964,4.006,2.081,1.031]]}}&#39; . If you see successful output, you have your first seldon-core-microservice up and running! . !docker container ls . CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e931762bb6d3 registry:2 &#34;/entrypoint.sh /etc‚Ä¶&#34; 44 hours ago Up 44 hours 0.0.0.0:5000-&gt;5000/tcp kind-registry .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/30/first-seldon-deployment.html",
            "relUrl": "/kubernetes/docker/2020/07/30/first-seldon-deployment.html",
            "date": " ‚Ä¢ Jul 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Exploring Kubernetes API",
            "content": "Kubernets API . The Kubernetes api is how all communication is passed to and within a kubernetes cluster. Any commands sent to kubernetes cluster with kubectl are hitting endpoints in the kubernetes cluster, which allow the cluster to make the appropriate changes. . Luckly for us, kubernetes makes it easy to explore the api through the use of kubectl proxy. In this post we will look through the different paths of the api to see how different functionalites are exposed . !kind create cluster --name explore . Creating cluster &#34;explore&#34; ... ‚úì Ensuring node image (kindest/node:v1.17.0) üñº ‚úì Preparing nodes üì¶ 7l ‚úì Writing configuration üìú7l ‚úì Starting control-plane üïπÔ∏è7l ‚úì Installing CNI üîå7l ‚úì Installing StorageClass üíæ7l Set kubectl context to &#34;kind-explore&#34; You can now use your cluster with: kubectl cluster-info --context kind-explore Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ . !kubectx kind-explore . Switched to context &#34;kind-explore&#34;. . !kubectl get pods . No resources found in default namespace. . With a new cluster up and running, go to a terminal, and end the command kubectl proxy --port=8000 (if port 8000 is taken, use a different number). You will want to run this command in a terminal because the server itself will need be running while we continue to execute commands from this notebook or your webrowser. . Kubectl proxy is a built-in kubernetes command that exposes the kubernetes api to the user. With a kubectl proxy server running locally, we can visit the different api paths to see information about what api endpoints are available. . To start exploring the kubernetes api, either go to http://127.0.0.1:8000/apis in your browser, or use a curl in this notebook: . !curl http://127.0.0.1:8000/apis . { &#34;kind&#34;: &#34;APIGroupList&#34;, &#34;apiVersion&#34;: &#34;v1&#34;, &#34;groups&#34;: [ { &#34;name&#34;: &#34;apiregistration.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;apiregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;apiregistration.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;apiregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;extensions&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;extensions/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;extensions/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;apps&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;apps/v1&#34;, &#34;version&#34;: &#34;v1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;apps/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;events.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;events.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;events.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;authentication.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;authentication.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;authentication.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;authentication.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;authorization.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;authorization.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;autoscaling&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;autoscaling/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;autoscaling/v2beta1&#34;, &#34;version&#34;: &#34;v2beta1&#34; }, { &#34;groupVersion&#34;: &#34;autoscaling/v2beta2&#34;, &#34;version&#34;: &#34;v2beta2&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;autoscaling/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;batch&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;batch/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;batch/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;batch/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;certificates.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;certificates.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;certificates.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;networking.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;networking.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;networking.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;networking.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;policy&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;policy/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;policy/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;rbac.authorization.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;rbac.authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;rbac.authorization.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;rbac.authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;storage.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;storage.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;storage.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;storage.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;admissionregistration.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;admissionregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;admissionregistration.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;admissionregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;apiextensions.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;apiextensions.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;apiextensions.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;apiextensions.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;scheduling.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;scheduling.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;scheduling.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;scheduling.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;coordination.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;coordination.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;coordination.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;coordination.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;node.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;node.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;node.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;discovery.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;discovery.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;discovery.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } } ] } . You&#39;ll see a big json reponse of kind APIGroupList. This response shows a list of kubernets APIGroups, which are a mechanism kubernetes uses to make it easier for users to extend the kubernetes api (We will see how the seldon custom resource definition extends this api by the end of this post!) . !kubectl create namespace tester . namespace/tester created . !kubens tester . Context &#34;kind-explore&#34; modified. Active namespace is &#34;tester&#34;. . !curl http://127.0.0.1:8000/api/v1 . { &#34;kind&#34;: &#34;APIResourceList&#34;, &#34;groupVersion&#34;: &#34;v1&#34;, &#34;resources&#34;: [ { &#34;name&#34;: &#34;bindings&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Binding&#34;, &#34;verbs&#34;: [ &#34;create&#34; ] }, { &#34;name&#34;: &#34;componentstatuses&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;ComponentStatus&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;list&#34; ], &#34;shortNames&#34;: [ &#34;cs&#34; ] }, { &#34;name&#34;: &#34;configmaps&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ConfigMap&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;cm&#34; ], &#34;storageVersionHash&#34;: &#34;qFsyl6wFWjQ=&#34; }, { &#34;name&#34;: &#34;endpoints&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Endpoints&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;ep&#34; ], &#34;storageVersionHash&#34;: &#34;fWeeMqaN/OA=&#34; }, { &#34;name&#34;: &#34;events&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Event&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;ev&#34; ], &#34;storageVersionHash&#34;: &#34;r2yiGXH7wu8=&#34; }, { &#34;name&#34;: &#34;limitranges&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;LimitRange&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;limits&#34; ], &#34;storageVersionHash&#34;: &#34;EBKMFVe6cwo=&#34; }, { &#34;name&#34;: &#34;namespaces&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Namespace&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;ns&#34; ], &#34;storageVersionHash&#34;: &#34;Q3oi5N2YM8M=&#34; }, { &#34;name&#34;: &#34;namespaces/finalize&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Namespace&#34;, &#34;verbs&#34;: [ &#34;update&#34; ] }, { &#34;name&#34;: &#34;namespaces/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Namespace&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;nodes&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Node&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;no&#34; ], &#34;storageVersionHash&#34;: &#34;XwShjMxG9Fs=&#34; }, { &#34;name&#34;: &#34;nodes/proxy&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;NodeProxyOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;nodes/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Node&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;persistentvolumeclaims&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;pvc&#34; ], &#34;storageVersionHash&#34;: &#34;QWTyNDq0dC4=&#34; }, { &#34;name&#34;: &#34;persistentvolumeclaims/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;persistentvolumes&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;PersistentVolume&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;pv&#34; ], &#34;storageVersionHash&#34;: &#34;HN/zwEC+JgM=&#34; }, { &#34;name&#34;: &#34;persistentvolumes/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;PersistentVolume&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;pods&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Pod&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;po&#34; ], &#34;categories&#34;: [ &#34;all&#34; ], &#34;storageVersionHash&#34;: &#34;xPOwRZ+Yhw8=&#34; }, { &#34;name&#34;: &#34;pods/attach&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodAttachOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/binding&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Binding&#34;, &#34;verbs&#34;: [ &#34;create&#34; ] }, { &#34;name&#34;: &#34;pods/eviction&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;group&#34;: &#34;policy&#34;, &#34;version&#34;: &#34;v1beta1&#34;, &#34;kind&#34;: &#34;Eviction&#34;, &#34;verbs&#34;: [ &#34;create&#34; ] }, { &#34;name&#34;: &#34;pods/exec&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodExecOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/log&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Pod&#34;, &#34;verbs&#34;: [ &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/portforward&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodPortForwardOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/proxy&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodProxyOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;pods/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Pod&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;podtemplates&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodTemplate&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;storageVersionHash&#34;: &#34;LIXB2x4IFpk=&#34; }, { &#34;name&#34;: &#34;replicationcontrollers&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ReplicationController&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;rc&#34; ], &#34;categories&#34;: [ &#34;all&#34; ], &#34;storageVersionHash&#34;: &#34;Jond2If31h0=&#34; }, { &#34;name&#34;: &#34;replicationcontrollers/scale&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;group&#34;: &#34;autoscaling&#34;, &#34;version&#34;: &#34;v1&#34;, &#34;kind&#34;: &#34;Scale&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;replicationcontrollers/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ReplicationController&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;resourcequotas&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ResourceQuota&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;quota&#34; ], &#34;storageVersionHash&#34;: &#34;8uhSgffRX6w=&#34; }, { &#34;name&#34;: &#34;resourcequotas/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ResourceQuota&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;secrets&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Secret&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;storageVersionHash&#34;: &#34;S6u1pOWzb84=&#34; }, { &#34;name&#34;: &#34;serviceaccounts&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ServiceAccount&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;sa&#34; ], &#34;storageVersionHash&#34;: &#34;pbx9ZvyFpBE=&#34; }, { &#34;name&#34;: &#34;services&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Service&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;svc&#34; ], &#34;categories&#34;: [ &#34;all&#34; ], &#34;storageVersionHash&#34;: &#34;0/CO1lhkEBI=&#34; }, { &#34;name&#34;: &#34;services/proxy&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ServiceProxyOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;services/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Service&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] } ] } .",
            "url": "https://ntorba.github.io/writing/kubernetes/api/2020/07/27/exploring-kubernetes-api.html",
            "relUrl": "/kubernetes/api/2020/07/27/exploring-kubernetes-api.html",
            "date": " ‚Ä¢ Jul 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Depth over Breadth",
            "content": "This is an idea I first explicitly learned about while listening to The Art of Learning by Josh Waitzkin. The main idea is that you can gain a much better understanding of the bigger picture (any object or field of study) by digging deep on a seemingly small part. For example, as a software engineer, depth over breadth would be dedicating your time to a single programming language instead of trying to learn 3 at once. A deep understanding of a single language improves your general programming ability more than a shallow understanding of many. Not to mention, learning new languages after a deep understanding of one opens the door to many important, nuanced connections. . Digging into the small part is what Josh calls the micro. ‚ÄúDepth over breadth‚Äù is understanding the macro (programming in general) from the micro (a single programming language). . A great example Josh uses in the book is from The Art of Motorcycle Maintenance. The main character, Phadreus, is a professor at a college in Bozeman, Montana. He teaches literature and writing. At one point, he has a notably hard working student completely stumped with writer‚Äôs block. The assignment is to write 500 words about Bozeman, a small town in rural Montana. Despite her determination, she just couldn‚Äôt get any words on the page. After many attempts to help, Phradreus frustratedly tells her ‚ÄúNarrow it down to the front of one building on the main street of Bozeman. The Opera House. Start with the upper left-hand brick.‚Äù The next day, the girl turned in a 5000 word essay. This point of view gave her endless inspiration. Focusing on something so small (the micro) gave her an entirely new view of the whole town (the macro). . This idea takes our original software analogy even further. Forget focusing on an entire language. Focus on a single library. Tear it apart. Use the debugger and step through all levels of the code. Look at how the authors abstracted their ideas. Analyze the data structures. Look for the use of language specific features. Contribute to it. . Deep understanding of a single library does much more than just help you understand that library. As you use others, you begin to see important connections, or striking differences. Those connections help you pick up new libraries much faster. You also begin to build a much deeper understanding of the language itself. . With a strategy like this, you build a deeper understanding of the macro (a programming language, or even programming itself) by narrowing in on the micro (a specific library of a single language). .",
            "url": "https://ntorba.github.io/writing/markdown/learning/learning%20strategies/growth/2020/07/24/depth-over-breadth.html",
            "relUrl": "/markdown/learning/learning%20strategies/growth/2020/07/24/depth-over-breadth.html",
            "date": " ‚Ä¢ Jul 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Custom Resources and Operators",
            "content": "Intro . In this post we will walk through the basics behind the seldon custom resource definition. At a high level, kubernetes only job is to maintain the desired state of the cluster. All interactions are changes to the desired state. The magic of kubernetes is that once you tell it the new state, it creates and maintains that state for you. The seldon-core projects serves inferences graphs on kubernetes with a custom resource and operator. These provide automation of complex systems, while allowing us to easily configure the desired state of our deployments without doing a lot of manual work. . Launch Cluster . To get started, let&#39;s get a cluster up and running. If you followed part 1 of this series, you can do so with kind. Below, I create a cluster, create a namespace called seldon-intro, then use kubens to make seldon-into my default namespace (so I don&#39;t have to include it in every command). . !kind create cluster !kubectl create namespace seldon-intro !kubens seldon-intro . Creating cluster &#34;kind&#34; ... ‚úì Ensuring node image (kindest/node:v1.17.0) üñº7l ‚úì Preparing nodes üì¶ 7l ‚úì Writing configuration üìú7l ‚úì Starting control-plane üïπÔ∏è7l ‚úì Installing CNI üîå7l ‚úì Installing StorageClass üíæ7l Set kubectl context to &#34;kind-kind&#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ . KubeAPI, Custom Resources, and Operators . The KubeAPI is the medium through which all communication is handled in a kubernetes cluster. It is a rest server. When you send commands to a kubernetes cluster, you are hitting a specific api endpoint with commands for the server to execute. Kubernetes comes with some built-in objects you should be familiar with. Deployments, services, pods, etc. These objects are useful in and of themselves, but we often need to use many of them at once, which can get cumbersome. For those familiar with seldon, you know that you can create very complex inference graphs with many components. Instead of deploying and connecting all seldon services manually, we are able to build a single json/yaml configuration that deploys the entire graph. This is possible because of operators and custom resources. Operators and custom resources have an intimate relationship, and must be used in tandem. Earlier, I introduce the KubeAPI. We know that is how all internal and external communication is handled in the cluster. With operators, we extend the KubeAPI. In other words, operators allow us to add more endpoints to the KubeApi to carry out custom commands in our cluster. Which is where custom resources come in. They define the custom instructions for our new endpoint to execute. Let&#39;s see an operator and custom resource in action. . Install Seldon-core . I suggest using helm to install seldon-core. If you haven&#39;t used helm before, use this page to find install instructions. (If you&#39;re on a mac, just use brew install helm). For those familiar with python, helm is like the pip for kubernetes. It works by using helm charts. A chart is a group of files that describe a higher level application using built-in kubernetes resources. For example, you could use a helm chart to deploy a full stack web application. We are going to use helm to install seldon-core and seldon-core-operator. These helm charts are what will allow us to deploy seldon inference graphs. You can learn more about helm charts here. . Once helm is installed, use it to install seldon-core and seldon-core-operator with the following command: . !helm install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --set usageMetrics.enabled=true --set ambassador.enabled=true --namespace seldon-intro #unnecessary after using `kubens seldon-intro`, but keeping here to make sure the install is explicit . NAME: seldon-core LAST DEPLOYED: Mon Jul 27 08:32:08 2020 NAMESPACE: seldon-intro STATUS: deployed REVISION: 1 TEST SUITE: None . After a successful install of seldon-core and seldon-core-operator, run the following command: . !kubectl get deployments . NAME READY UP-TO-DATE AVAILABLE AGE seldon-controller-manager 1/1 1 1 6s . We see we now have a deployment, seldon-controller-manager, running in our seldon-intro namespace. . !!kubectl describe svc seldon-webhook-service . [&#39;Name: seldon-webhook-service&#39;, &#39;Namespace: seldon-intro&#39;, &#39;Labels: app=seldon&#39;, &#39; app.kubernetes.io/instance=seldon-core&#39;, &#39; app.kubernetes.io/managed-by=Helm&#39;, &#39; app.kubernetes.io/name=seldon-core-operator&#39;, &#39; app.kubernetes.io/version=1.2.1&#39;, &#39;Annotations: meta.helm.sh/release-name: seldon-core&#39;, &#39; meta.helm.sh/release-namespace: seldon-intro&#39;, &#39;Selector: app.kubernetes.io/instance=seldon1,app.kubernetes.io/name=seldon,app.kubernetes.io/version=v0.5,app=seldon,control-plane=seldon-controller-manager&#39;, &#39;Type: ClusterIP&#39;, &#39;IP: 10.96.113.42&#39;, &#39;Port: &lt;unset&gt; 443/TCP&#39;, &#39;TargetPort: 443/TCP&#39;, &#39;Endpoints: 10.244.0.5:443&#39;, &#39;Session Affinity: None&#39;, &#39;Events: &lt;none&gt;&#39;] . The docker image name used to create the seldon-controller-manager pod and deployment is called docker.io/seldonio/seldon-core-operator:1.2.1. That is because the seldon-controller-manager is the seldon-core-operator. . Be sure to match the last line with the namespace you created in the previous command. We will talk more about what the other lines mean later in the series. After installing, run . !kubectl get pods -o wide . and you&#39;ll see you have a pod running with seldon-controller-manager in the name (followed by a random string, we will talk about why that is soon). Next, run . !kubectl get deployments . Seldon-controller-manager is a kubernetes operator. As described here, kubernetes operators are extensions of the kubernetes api that allow users to easily package and deploy complex applications on kubernetes. The seldon-core-operator adds a lot of functionality to the kubernetes cluster and allows us to interact with seldon deployments as if they were a built-in kubernetes object. Along with this, we also installed the seldon-core custom resource definition. Kubernetes custom resources are extensions of the native kuberetes api. As you will see soon, we will now be able to deploy and interact with a new structure, the sdep, in the same fashion we deploy and monitor kubernetes deployments, servives, and other built-in resources. . All interactions with a kubernetes cluster is the user specifying a new desired state, and the kubernetes cluster using it&#39;s resources to change into that desired state. When, a custom resource is created, an operator is needed as well to tell kubernetes how to handle updates to the desired state of the custom resource. The seldon-core operator is what allows users to make edits to currently running seldon deployments without downtime. . Operators allows kubernetes to run stateful applications. A popular usecase for operators is databases. . To get a better idea at how helm charts add to your kubernetes cluster, check out the Exploring the Kubernetes API post digs into the basic internals of how your cluster actually receives commands. . In this post, they write . If you had to sum up Kubernetes in a word, the best choice might not be ‚Äúorchestration‚Äù but ‚Äúautomation.‚Äù That‚Äôs what it‚Äôs all about:Kubernetes enables the automation of the infrastructure (and corresponding operational burden of managing that infrastructure) necessary for running containerized applications ‚Äì a must when running these apps at scale in production environments. This is very evident with the Seldon deployment CRD and operator. As we move along and start to create our own seldon deployments, you will see the power of this automation. With a single file, a seldon deployment will launch multiple services, deployments, pods, and allow for continuous updates of all those components through the Kubernetes api. Seldon has leveraged the power of the automation offered by kubernetes to create inferences graphs of many components. . Another great quote from that article: . ‚ÄúOperators are simplifying the process highly complex distributed database management by defining the installation, scale, updates, and management lifecycle of a stateful clustered application,‚Äù says Yossi Jana, DevOps team leader at AllCloud. From another vantage point, consider life without Operators. ‚ÄúWithout Operators, many applications need intervention to deploy, scale, reconfigure, upgrade, or recover from faults,‚Äù Thompson says. ‚ÄúIf your app ‚Äì or apps that you depend on, such as your database management system ‚Äì [requires] DevOps engineers hovering over a keyboard in these critical moments, hoping they get the steps correctly, you‚Äôre almost certain to have greater downtime and more stress in your team.‚Äù . From kubernetes-operator-sdk tutorial, operators are used to define custom resources. They extend the kubernetes api to tell the cluster how to handle those resources. Operators themselves run in pods. This is why you see the seldon-controller-manager deployment and pod running after we install seldon-core with helm. . As described in the kubernetes docs here, operators follow the controller pattern, which means they are responsible for keep the desired state of the custom resource they are responsible for. The seldon-core-operator is responsbile for the Seldon Deployment custom resource. That means, when we create or edit a seldon deployment, the seldon-core-operator is responsible for adjusting the kubernetes deployment to the desired state to meet the new edits applied by the user. . In every article you read about kubernetes operators, you&#39;ll find some sentiment to the fact that is confusing the first time around. In fact, pretty much everything you learn about kubernetes will be confusing the first time around. Don&#39;t let that discourage you. That is why we are using seldon to help our understanding. Instead of reading description after description of what a custom resource and operator are, let&#39;s actually use them to begin to understand the power they provide. . Let&#39;s launch our first seldon deployment onto the cluster to see what the consequences of creating a seldon deployment custom resource are. . %%bash kubectl apply -f - &lt;&lt; END apiVersion: machinelearning.seldon.io/v1alpha2 kind: SeldonDeployment metadata: name: iris-model spec: name: sklearn-iris-deployment predictors: - componentSpecs: - spec: containers: - image: seldonio/sklearn-iris:0.1 imagePullPolicy: IfNotPresent name: sklearn-iris-classifier graph: children: [] endpoint: type: REST name: sklearn-iris-classifier type: MODEL name: predictor replicas: 1 END . !kubectl get pods .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/20/dive-into-operators-and-custom-resources.html",
            "relUrl": "/kubernetes/docker/2020/07/20/dive-into-operators-and-custom-resources.html",
            "date": " ‚Ä¢ Jul 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Cluster Interaction Basics",
            "content": "Intro . This post walks through some kubernetes basics and launches your first deployment and service on your cluster. .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/18/cluster-interaction-basics.html",
            "relUrl": "/kubernetes/docker/2020/07/18/cluster-interaction-basics.html",
            "date": " ‚Ä¢ Jul 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Local Kubernetes on KIND",
            "content": "Intro . This is part one of a series learning about kubernetes through seldon-core. In this post, I will introduce, Kind, an awesome tool to run a kubernetes cluster on your local computer and some basic commands to interact with the cluster (I highly suggest downloading kubectx and kubens!) If you are brand new to kubernetes, I highly reccomend you first go follow the kubernetes interactive tutorial to get a handle on basic interactions. Then, come back here to have some fun on your own cluster. If you already have access to a kubernetes cluster and want to lean about seldon-core, check out part 2! . Install kubectl . If this is the first time you&#39;ve used kubernetes, you will need to install kubectl, the command line tool for interacting with kubernetes. This can be downloaded here. On mac you can use brew install kubectl. Check your install by running: . !kubectl version --client --short . Client Version: v1.18.5 . If you see similar output, you are good to go! . Install Kind . If you&#39;re on mac, it&#39;s as simple as brew install kind. If not, check out this page . Create your First Cluster . !kind create cluster . Creating cluster &#34;kind&#34; ... ‚úì Ensuring node image (kindest/node:v1.17.0) üñº ‚úì Preparing nodes üì¶ 7l ‚úì Writing configuration üìú7l ‚úì Starting control-plane üïπÔ∏è7l ‚úì Installing CNI üîå7l ‚úì Installing StorageClass üíæ7l Set kubectl context to &#34;kind-kind&#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Thanks for using kind! üòä . It&#39;s as simple as that. If it is your first time running kind, it will automatically download the appropiate docker image (something like kindest/node:1.17.0), which may take a few minutes. After that command is finished, check if your cluster is running: . !kubectl cluster-info . Kubernetes master is running at https://127.0.0.1:32771 KubeDNS is running at https://127.0.0.1:32771/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;. . If you see output like above, displaying info about your Kubernetes master and KubeDNS, then you have successfully launched a local kubernetes cluster! . Bonus: Install kubectx and kubens . As you follow through the next posts, I will be using the kubectx and kubens command line tools. If you are on mac, you can install them with brew: brew install kubectx. This will download and install both kubectx and kubens. If you&#39;re not on mac, find install instructions here. These allow you to easily switch between kubernetes contexts and namespaces. You can perform all the same actions with kubectl, but kubectx and kubens make some common commands much quicker. .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/17/local-kubernetes.html",
            "relUrl": "/kubernetes/docker/2020/07/17/local-kubernetes.html",
            "date": " ‚Ä¢ Jul 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Learning Kubernetes Through Seldon",
            "content": "Intro . I recently started working with with the seldon-core project. I am working on a team now that is developing an ML platform to allower researchers to quickly serve and monitor ML models at scale. Seldon-core is a fantastic tool to build on top of for this project. Seldon-core deploys models at scale by leveraging kubernetes. The only problem was, I had never used kubernetes before. . Launch a local kubernetes cluster. Follow this guide to get a local kubernetes cluster up and running so you can experiment locally. Skip this section if you already have access to a kubernetes cluster. | First Seldon Deployment | Seldon Deployments: Custom Resources on Kubernetes. Create your first seldon deployment and read about what makes the seldon deployment custom resource so useful. | Debugging Seldon Deployments. Take a look at where to find your logs and diagnosis some common issues. | Seldon-core analytics and load testing with Locust | Advanced Seldon Deployments. Explore deploying to multiple pods and defining Horizontal Pod Autoscaling. | CD with Argocd. Explore how to use argocd to continuously deploy. | Why Seldon-core? . Seldon-core is an open source projects built by the London based startup Seldon. With seldon-core, you can use python (and java) to easily deploy ML models built in any framework at scale. However, they offer tools for more than just model serving. With seldon-core, you construct an inference graph. The inference graph is built with these components: * Model * Transformer * Combiner * Router These additional components add the ability to create much more than just a single model. You can set up custom A/B tests, or 1-armed bandit systems with the router, combine results from two different models with the combiner, and have independent components (which allows separate scale settings) to transform the input data. Seldon-core inference graphs are powerful! . What‚Äôs this series? . In order to take advantage of seldon-core, you need to become familiar with some basic kubernetes fundamentals, so that‚Äôs what I did. First, I followed the kubernetes interactive tutorial to get a handle. Then, I dove right into seldon deployments. Working with seldon-core and building implementations of projects is a great way to get the kubernetes basics down. At the start, I couldn‚Äôt tell you the difference between a kubernetes deployment and a service. Although I‚Äôm not an expert now, I‚Äôm comfortable with seldon deployments, all the components they run on top of, and how to quickly debug my implementations by checking through the logs of containers scattered throughout the different components of my seldon deployments. In this post, I link to, in order, posts I‚Äôve written detailing the concepts and tools I learned to become confident in building and deploying at scale with seldon-core. .",
            "url": "https://ntorba.github.io/writing/markdown/seldon/kubernetes/python/2020/07/17/learning-kubernetes-through-seldon.html",
            "relUrl": "/markdown/seldon/kubernetes/python/2020/07/17/learning-kubernetes-through-seldon.html",
            "date": " ‚Ä¢ Jul 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Good At Math Bad At Writing",
            "content": "Good at Math, Bad at Writing . The biggest failure of my education was the acceptance of the ‚Äúgood at math, bad at writing‚Äù label. It comes as no surprise that I ended up working as a software engineer, but it took until now to see how much time I wasted hating writing, when in fact, the underlying principles are strikingly similar. . Looking at the end product, the similarities are hard to see. Books and programs are much different, but the process to generate them is quite similar. For both, the ultimate goal is to give the user/reader (‚Äúuser‚Äù for software and ‚Äúreader‚Äù for writing - they serve the same purpose) a particular experience by providing information in a well thought out structure. This goal is achieved using a set of tools and conventions. Software leverages programming languages with different designs (object oriented vs functional, for example) and frameworks (web application vs data application). Writing leverages spoken language with accepted structures such as short-stories, books, long-form, and poems. In both, deciding the general structure and design (picking a programming language vs picking a writing structure) is integral to creating the end experience you wish to generate. . Software engineers specialize in particular niches similar to authors specializing in particular structures. I build data applications with python to improve the experience of data scientists while poets use poems to create an experience for their readers. In the end, the best software engineers and writers are those who most effectively organize and present necessary information to their audience. . It is sometimes easy to miss when this is done well, but never hard to miss when done poorly. Everyone knows the absolute frustration of using software that is particularly difficult to understand or flat out doesn‚Äôt do what its supposed to. While bad writing leaves you lost, or worse, leaves you in apparent understanding, only later finding you completely missed the point the author wanted to get across (something I‚Äôve had happen in my writing many a time). Both of these are results of bad information organization. At some point the creators mis executed along their path to creating the experience they planned on. . Most disciplines share the same underlying principles. These hidden connections is what makes me so sad about the all too common ‚ÄúGood at math, bad at writing‚Äù label given to so many students at such a young age. It is harmful to plant this idea in a young mind, blocking them from the hidden connections they then don‚Äôt bother to look for. I‚Äôve come to realize how important and enjoyable writing is in my everyday life. Not only do you need to write more than ever to communicate with work colleagues via slack and email, but nothing can help boost memory more than writing well thought out notes about new topics and ideas. . I‚Äôm angry at any teacher who allowed this idea to propagate or ever did a shitty job teaching an English class. English teachers have such a great opportunity to showcase how important writing is to every aspect of life and they squandered it. . This realization has been very important to me. I will forever despise the ‚Äúgood at math, bad at writing‚Äù because of the limitations it tricks people into thinking they have. .",
            "url": "https://ntorba.github.io/writing/2020/05/07/Good-at-Math-Bad-At-Writing.html",
            "relUrl": "/2020/05/07/Good-at-Math-Bad-At-Writing.html",
            "date": " ‚Ä¢ May 7, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Exploring Metaflow",
            "content": "About . metaflow is a python package open sourced by netflix to help data scientists easily scale their project workflows. Metaflow is mainly interacted with through decorators. In this post, we will get behind the scenes of how these decorators actually work. . The code . To start, let&#39;s take a look at the first example in the documentation. This is a simple flow. . from metaflow import FlowSpec, step class LinearFlow(FlowSpec): @step def start(self): self.my_var = &#39;hello world&#39; self.next(self.a) @step def a(self): print(&#39;the data artifact is: %s&#39; % self.my_var) self.next(self.end) @step def end(self): print(&#39;the data artifact is still: %s&#39; % self.my_var) LinearFlow() . We see that the LinearFlow python class inherits from metaflow&#39;s FlowSpec class, and each of the functions are decorated with @step. As seen (here)[https://docs.metaflow.org/metaflow/basics], this basic flow follows metaflow&#39;s guidelines. However, what is actually happening? How does it turn our functions into pipeline steps? Let&#39;s start by taking a look at the Flowspec class. . (Flowspec)[https://github.com/Netflix/metaflow/blob/master/metaflow/flowspec.py] definition and constructor. Full code can be found at the link. . class FlowSpec(object): &quot;&quot;&quot; Main class from which all Flows should inherit. Attributes - script_name index input &quot;&quot;&quot; # Attributes that are not saved in the datastore when checkpointing. # Name starting with &#39;__&#39;, methods, functions and Parameters do not need # to be listed. _EPHEMERAL = {&#39;_EPHEMERAL&#39;, &#39;_datastore&#39;, &#39;_cached_input&#39;, &#39;_graph&#39;, &#39;_flow_decorators&#39;, &#39;_steps&#39;, &#39;index&#39;, &#39;input&#39;} _flow_decorators = {} def __init__(self, use_cli=True): &quot;&quot;&quot; Construct a FlowSpec Parameters - use_cli : bool, optional, default: True Set to True if the flow is invoked from __main__ or the command line &quot;&quot;&quot; self.name = self.__class__.__name__ self._datastore = None self._transition = None self._cached_input = {} self._graph = FlowGraph(self.__class__) self._steps = [getattr(self, node.name) for node in self._graph] if use_cli: # we import cli here to make sure custom parameters in # args.py get fully evaluated before cli.py is imported. from . import cli cli.main(self) .",
            "url": "https://ntorba.github.io/writing/jupyter/2020/03/08/metaflow-exploration.html",
            "relUrl": "/jupyter/2020/03/08/metaflow-exploration.html",
            "date": " ‚Ä¢ Mar 8, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Figuring Out The Brain",
            "content": "Intro . Mathew Cobb recently wrote Why your brain is not a computer on The Guardian. Thank you to Data Science Weekly and Data Exlixir newsletters for sending both sending this to me. . Summary . In this article, Cobb details some of history of the ‚Äúbrain as a computer metaphor.‚Äù With the rise of AI, this topic has become very relevant. There are many smart people in many different camps, and its hard to figure out who has the best information when there is so much out there. . My Thoughts . While reading this, an idea from a software engineering article I recently read struck me: ‚ÄúIt is easier to write code than to read it.‚Äù The consequences of this fact are littered throughout code bases and can help us understand our struggle to understand the brain. . Reading code is a form of reverse engineering. Working in the reverse direction is often much more difficult than building from the ground up. This is counter-intuitive until you experience it. Very commonly in software, problems that seem quicky and easy end up thorny and difficult. So, a motivated engineer sees a big jumble of old code for an ‚Äúeasy‚Äù problem and decides this other person must have been an idiot, so they decide to take their own stab at it, only to find that their new solution is a new fangled mess as well. . This situaiton is prominent in other work as well. Many tasks are forms of reverse engineering - reading even! It is easier to write and understand your own thoughts than to read and understand someone else‚Äôs. So, if we humans find reverse engineering so difficult for these everyday tasks, its not wonder its been such a nightmare trying to reverse engineer the brain (I guess it‚Äôs a shame evolution isn‚Äôt a better author). It is no wonder our main metaphor for the brain is computers. We built computers and use them everyday. They are the closest analogue we have, although they are quite different. . This line of thought puts me firmly in the ‚Äúbetter metaphor for understanding‚Äù category. The only chance humans have to understand the brain is to continue to try to build our own version then compare the outcome. . If we were given a modern computer 100 years ago, would it have advanced the rate of technology? I suppose not. It would have been so advanced that the people of the times wouldn‚Äôt have known where to start reverse engineering such a complex technology. So it is with the brain. It has somehow put itself far ahead of our time, out of reach of reverse engineering, a skill we so desperately wish we were proficient at for this task. .",
            "url": "https://ntorba.github.io/writing/markdown/philosophy/first%20draft/2020/03/07/figuring-out-the-brain.html",
            "relUrl": "/markdown/philosophy/first%20draft/2020/03/07/figuring-out-the-brain.html",
            "date": " ‚Ä¢ Mar 7, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://ntorba.github.io/writing/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 . Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://ntorba.github.io/writing/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "About Me . Trying to get better at writing because it seems like a good thing to get good at. .",
          "url": "https://ntorba.github.io/writing/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}