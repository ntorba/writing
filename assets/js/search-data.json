{
  
    
        "post0": {
            "title": "Title",
            "content": "",
            "url": "https://ntorba.github.io/writing/2020/08/24/Untitled.html",
            "relUrl": "/2020/08/24/Untitled.html",
            "date": " • Aug 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Launch a Seldon Deployment",
            "content": "Reqs . access to kubernetes cluster If you are coming from Launch a local kubernetes cluster, you are good to follow this example. If not, you can quickly follow that post before running the example here! | . | . Goal . Launch first seldon deployment with grpc or rest | . Steps . Define a seldon python component | Build docker image | Run a container based on docker image to test the endpoint | Define SeldonDeployment yaml file | kubectl apply SeldonDeployment to the kubernetes cluster. | Define Python Component . I&#39;m taking this example code directly from seldon-core irisClassifier example. First, we train a model based on the iris dataset included in the sklearn package, then we serve that trained model in the seldon endpoint. . #hide_output !mkdir iris_classifier . %%writefile iris_classifier/train_iris.py #collapse_show #hide_output import joblib from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn import datasets OUTPUT_FILE = &quot;iris_classifier/IrisClassifier.sav&quot; print(&quot;Loading iris data set...&quot;) iris = datasets.load_iris() X, y = iris.data, iris.target print(&quot;Dataset loaded!&quot;) clf = LogisticRegression(solver=&quot;liblinear&quot;, multi_class=&quot;ovr&quot;) p = Pipeline([(&quot;clf&quot;, clf)]) print(&quot;Training model...&quot;) p.fit(X, y) print(&quot;Model trained!&quot;) print(f&quot;Saving model in {OUTPUT_FILE}&quot;) joblib.dump(p, OUTPUT_FILE) print(&quot;Model saved!&quot;) . . Overwriting iris_classifier/train_iris.py . #hide_output !python iris_classifier/train_iris.py . Next, we define the seldon python component that will be used to serve the model. Seldon has a few components. In this example, we only use the Model component. Seldon components hold the logic that will be implanted into the serving endpoint that seldon creates. The model component must have a predict function, which is called when the future endpoint is hit. The reason seldon is so useful is because this is the only python code we need to write to serve this model. Seldon provides the rest of the logic, which puts this component into a web server, to serve the model. . An important note about this section is that you&#39;lll see the file is named IrisClassifier.py, which is camelcased. This is important, and you should not change this. The file name and the python component class name must match. . %%writefile iris_classifier/IrisClassifier.py #collapse_show #hide_output import joblib class IrisClassifier(object): def __init__(self): self.model = joblib.load(&#39;IrisClassifier.sav&#39;) def predict(self,X,features_names): return self.model.predict_proba(X) . . Overwriting iris_classifier/IrisClassifier.py . Build Docker Image . After defining a python component, there are two ways to create the docker image necessary for deployment. . define a Dockerfile which launches the seldon microservice | use s2i to build the image directly from source code. | . I prefer manually defining a Dockerfile because it provides more control over the process. However, s2i is a great tool that works just as well. . Write requirements.txt . We must write a requirements.txt library with all requirements for the docker image listed. . %%writefile iris_classifier/requirements.txt #hide_output sklearn seldon-core . Define Dockerfile . The Dockerfile follows the example provided here. We start from the python:3.7-slim base image, copy the code from the current directory, which includes the python component we defined earlier, install requirements, then expose port 5000 for the microservice to run. Next, we define seldon specific variables. . MODEL_NAME must match the python file name (which also much match the python component class name). | API_TYPE can be either REST or GRPC. | SERVICE_TYPE is the type of seldon component. MODEL for this example. (explore the other seldon components here | PERSISTENCE: 0 or 1. Defaults to 0. If it is set to 1, the component class will be periodically persisted to reis. This s unnecessary for our case because the component class will not change. this would be more pertinent for components like routers, which can have updating states for long running jobs. | . | . %%writefile iris_classifier/Dockerfile #collapse_show #hide_output FROM python:3.7-slim COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 5000 # Define environment variable ENV MODEL_NAME IrisClassifier ENV API_TYPE REST ENV SERVICE_TYPE MODEL ENV PERSISTENCE 0 # seldon-core-microservice is a command line tool installed with the seldon-core python libray. You can use this locally as well! CMD exec seldon-core-microservice $MODEL_NAME $API_TYPE --service-type $SERVICE_TYPE --persistence $PERSISTENCE . . Overwriting iris_classifier/Dockerfile . To test this example, let&#39;s build and run the docker image! . Docker Build . Pass the iris_classifier dir where the image guts live, then pass a -t to tag the image with a name referring to your preferred docker image repository (I&#39;m running on locally). . #hide_output !docker build iris_classifier/ -t localhost:5000/iris_ex:latest . Test Image . You can test your newly created image by running the image and hitting the endpoint. You may ask yourself at this point, &quot;if I have a working docker image, what do I need kubernetes for?&quot; This is a great question. For simple use cases, this docker image itself is all you need, and you could run it as a standalone service. If the load is small and you can run it without any load balancing functionalities, you are good to go. However, kubernetes is a container orchestration engine. That means it is built to handle complex containerized applications and will make your life much easier if you need to handle more complex operations for applications that need to serve on a large scale. . !docker run --name &quot;iris_predictor&quot; -d --rm -p 5001:5000 localhost:5000/iris_ex:latest . 4d88f1163a71622fc2b67f33b8af4e95c2c8dafa9da43e2fe8c06e4322b7591c . You could also remove the -d argument from the above command and run this command in a separate window to see the log output while sending requests to the endpoint. Test the endpoint with the curl below! . import numpy as np import grpc from seldon_core.proto import prediction_pb2 from seldon_core.proto import prediction_pb2_grpc ### Test Rest Endpoint !curl -s http://localhost:5001/predict -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;data&quot;:{&quot;ndarray&quot;:[[5.964,4.006,2.081,1.031]]}}&#39; ### Test GRPC Endpoint # data = np.array([[5.964,4.006,2.081,1.031]]) # datadef = prediction_pb2.DefaultData( # tensor=prediction_pb2.Tensor(shape=data.shape, values=data.flatten()) # ) # request = prediction_pb2.SeldonMessage(data=datadef) # with grpc.insecure_channel(&quot;localhost:5001&quot;) as channel: # stub = prediction_pb2_grpc.ModelStub(channel) # response = stub.Predict(request=request) # print(response) . If you see successful output, you have your first seldon-core-microservice up and running! Now, we will deploy this as a simple inference graph on our kubernetes cluster. First, let&#39;s take down the running docker container: . Next, need to define our deployment configuration file. Here is a seldon config file for our deployment: . !docker container rm iris_predictor --force . iris_predictor . %%writefile iris_classifier/sklearn_iris_deployment.yaml #hide_output apiVersion: machinelearning.seldon.io/v1alpha2 kind: SeldonDeployment metadata: name: seldon-deployment-example spec: name: sklearn-iris-deployment predictors: - componentSpecs: - spec: containers: - image: seldonio/sklearn-iris:0.1 imagePullPolicy: IfNotPresent name: sklearn-iris-classifier graph: children: [] endpoint: type: REST name: sklearn-iris-classifier type: MODEL name: sklearn-iris-predictor replicas: 1 . Some important notes about the deployment config: . apiVersion: this sends out request to the appropriate endpoint of the kubernets api, which was installed by helm earlier in this tutorial | kind: tells Kubernetes what kind of resource to create. | metadata: add labels, like name, to the deployment | spec: predictors: this is a list of predictors to deploy. It is a list because you have the option to create multiple inference graphs in the same spec. This is useful for things like Canary deployment, where you only want a new graph to recieve a small percentage of traffic componentSpecs: add information about the containers that need to be pulled to create our graph. In our case, we only need a single containe to serve our model. If we were creating a more complex inference graph (maybe with a transformer, router, and another model, then we would need to include the docker containers that house them in this section) | graph: this is where you define the flow of components. This is easy in our case, there is only one component so we define one endpoint with no children. If there were more compnoents, we would fill out the children componenets in the children attriubte of the head of the graph. Seldon graphs are built implicitly through the use of the children attribute of each node in the graph. | . | . | . There is one last step to deploy our graph, we must push our docker container to a registry! I am running a local registry with my kind cluster, thanks to the script given here. You can also push to DockerHub as well. . !docker push localhost:5000/iris_ex:latest . The push refers to repository [localhost:5000/iris_ex] 60d57b93: Preparing 43291ec5: Preparing 63f2d025: Preparing f01300cf: Preparing a0be9040: Preparing 1a837902: Preparing 60d57b93: Pushed 276MB/269.5MBAlatest: digest: sha256:b100e77cc2fc9b8b22f043f5d2c061f4d85bf891d84e21641c0cec1844f345f2 size: 1792 . With our docker image in a registry, it is available to our cluster, so we can deploy! . !kubectl apply -f iris_classifier/sklearn_iris_deployment.yaml from time import sleep sleep(5) # give the clsuter some to get the deployment running before executing the rollout . seldondeployment.machinelearning.seldon.io/seldon-deployment-example created . You can check the status of your deployment. . !kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-deployment-example -o jsonpath=&#39;{.items[0].metadata.name}&#39;) . Waiting for deployment &#34;seldon-92a927e5e90d7602e08ba9b9304f70e8&#34; rollout to finish: 0 of 1 updated replicas are available... deployment &#34;seldon-92a927e5e90d7602e08ba9b9304f70e8&#34; successfully rolled out . Once the deployment is ready, you will need to port-forward the pod to your localhost in order check the request. That can be done wiht kubectl port-forward command . kubectl port-forward $(kubectl get pods -l seldon-app=seldon-deployment-example-sklearn-iris-predictor -o jsonpath=&#39;{.items[0].metadata.name}&#39;) 9000:9000 . You must run this command in a separate window because it will need to run while we curl the endpoint. . # dir(prediction_pb2_grpc) . import numpy as np import grpc from seldon_core.proto import prediction_pb2 from seldon_core.proto import prediction_pb2_grpc ### Test REST endpoint res = !curl -s http://localhost:9000/predict -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;data&quot;:{&quot;ndarray&quot;:[[5.964,4.006,2.081,1.031]]}}&#39; print(res) ### Test GRPC endpoint # data = np.array([[5.964,4.006,2.081,1.031]]) # datadef = prediction_pb2.DefaultData(tensor=prediction_pb2.Tensor(shape=data.shape, values=data.flatten())) # request = prediction_pb2.SeldonMessage(data=datadef) # with grpc.insecure_channel(&quot;localhost:9000/predict&quot;) as channel: # stub = prediction_pb2_grpc.ModelStub(channel) # print(dir(stub)) # response = stub.Predict(request=request) # print(response) . [&#39;{&#34;data&#34;:{&#34;names&#34;:[&#34;t:0&#34;,&#34;t:1&#34;,&#34;t:2&#34;],&#34;ndarray&#34;:[[0.9548873249364169,0.04505474761561406,5.792744796895234e-05]]},&#34;meta&#34;:{}}&#39;] . You have successfully created a seldon endpoint on kubernetes! . ## Cleanup !kubectl delete -f sklearn_iris_deployment.yaml . seldondeployment.machinelearning.seldon.io &#34;seldon-deployment-example&#34; deleted . Conclusion . In this quick example, we scratched the surface of seldon-core by deploying a simple model endpoint on kubernetes. If you are hungry for more, chech out more of the posts in the Seldon Super Series. There, you can find notebooks similar to this that deploy more complex inference graphs, or dive into the underlying kubernetes concepts that seldon uses to make this possible! . Next Up . other seldon components | seldon graph construction | multi-component inference graph | operators and custom resources | .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/30/first-seldon-deployment.html",
            "relUrl": "/kubernetes/docker/2020/07/30/first-seldon-deployment.html",
            "date": " • Jul 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Exploring Kubernetes API",
            "content": "Kubernets API . The Kubernetes api is how all communication is passed to and within a kubernetes cluster. Any commands sent to kubernetes cluster with kubectl are hitting endpoints in the kubernetes cluster, which allow the cluster to make the appropriate changes. . Luckly for us, kubernetes makes it easy to explore the api through the use of kubectl proxy. In this post we will look through the different paths of the api to see how different functionalites are exposed . !kind create cluster --name explore . Creating cluster &#34;explore&#34; ... ✓ Ensuring node image (kindest/node:v1.17.0) 🖼 ✓ Preparing nodes 📦 7l ✓ Writing configuration 📜7l ✓ Starting control-plane 🕹️7l ✓ Installing CNI 🔌7l ✓ Installing StorageClass 💾7l Set kubectl context to &#34;kind-explore&#34; You can now use your cluster with: kubectl cluster-info --context kind-explore Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂 . !kubectx kind-explore . Switched to context &#34;kind-explore&#34;. . !kubectl get pods . No resources found in default namespace. . With a new cluster up and running, go to a terminal, and end the command kubectl proxy --port=8000 (if port 8000 is taken, use a different number). You will want to run this command in a terminal because the server itself will need be running while we continue to execute commands from this notebook or your webrowser. . Kubectl proxy is a built-in kubernetes command that exposes the kubernetes api to the user. With a kubectl proxy server running locally, we can visit the different api paths to see information about what api endpoints are available. . To start exploring the kubernetes api, either go to http://127.0.0.1:8000/apis in your browser, or use a curl in this notebook: . !curl http://127.0.0.1:8000/apis . { &#34;kind&#34;: &#34;APIGroupList&#34;, &#34;apiVersion&#34;: &#34;v1&#34;, &#34;groups&#34;: [ { &#34;name&#34;: &#34;apiregistration.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;apiregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;apiregistration.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;apiregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;extensions&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;extensions/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;extensions/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;apps&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;apps/v1&#34;, &#34;version&#34;: &#34;v1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;apps/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;events.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;events.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;events.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;authentication.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;authentication.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;authentication.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;authentication.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;authorization.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;authorization.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;autoscaling&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;autoscaling/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;autoscaling/v2beta1&#34;, &#34;version&#34;: &#34;v2beta1&#34; }, { &#34;groupVersion&#34;: &#34;autoscaling/v2beta2&#34;, &#34;version&#34;: &#34;v2beta2&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;autoscaling/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;batch&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;batch/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;batch/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;batch/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;certificates.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;certificates.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;certificates.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;networking.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;networking.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;networking.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;networking.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;policy&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;policy/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;policy/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;rbac.authorization.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;rbac.authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;rbac.authorization.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;rbac.authorization.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;storage.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;storage.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;storage.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;storage.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;admissionregistration.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;admissionregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;admissionregistration.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;admissionregistration.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;apiextensions.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;apiextensions.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;apiextensions.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;apiextensions.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;scheduling.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;scheduling.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;scheduling.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;scheduling.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;coordination.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;coordination.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; }, { &#34;groupVersion&#34;: &#34;coordination.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;coordination.k8s.io/v1&#34;, &#34;version&#34;: &#34;v1&#34; } }, { &#34;name&#34;: &#34;node.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;node.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;node.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } }, { &#34;name&#34;: &#34;discovery.k8s.io&#34;, &#34;versions&#34;: [ { &#34;groupVersion&#34;: &#34;discovery.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } ], &#34;preferredVersion&#34;: { &#34;groupVersion&#34;: &#34;discovery.k8s.io/v1beta1&#34;, &#34;version&#34;: &#34;v1beta1&#34; } } ] } . You&#39;ll see a big json reponse of kind APIGroupList. This response shows a list of kubernets APIGroups, which are a mechanism kubernetes uses to make it easier for users to extend the kubernetes api (We will see how the seldon custom resource definition extends this api by the end of this post!) . !kubectl create namespace tester . namespace/tester created . !kubens tester . Context &#34;kind-explore&#34; modified. Active namespace is &#34;tester&#34;. . !curl http://127.0.0.1:8000/api/v1 . { &#34;kind&#34;: &#34;APIResourceList&#34;, &#34;groupVersion&#34;: &#34;v1&#34;, &#34;resources&#34;: [ { &#34;name&#34;: &#34;bindings&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Binding&#34;, &#34;verbs&#34;: [ &#34;create&#34; ] }, { &#34;name&#34;: &#34;componentstatuses&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;ComponentStatus&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;list&#34; ], &#34;shortNames&#34;: [ &#34;cs&#34; ] }, { &#34;name&#34;: &#34;configmaps&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ConfigMap&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;cm&#34; ], &#34;storageVersionHash&#34;: &#34;qFsyl6wFWjQ=&#34; }, { &#34;name&#34;: &#34;endpoints&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Endpoints&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;ep&#34; ], &#34;storageVersionHash&#34;: &#34;fWeeMqaN/OA=&#34; }, { &#34;name&#34;: &#34;events&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Event&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;ev&#34; ], &#34;storageVersionHash&#34;: &#34;r2yiGXH7wu8=&#34; }, { &#34;name&#34;: &#34;limitranges&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;LimitRange&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;limits&#34; ], &#34;storageVersionHash&#34;: &#34;EBKMFVe6cwo=&#34; }, { &#34;name&#34;: &#34;namespaces&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Namespace&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;ns&#34; ], &#34;storageVersionHash&#34;: &#34;Q3oi5N2YM8M=&#34; }, { &#34;name&#34;: &#34;namespaces/finalize&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Namespace&#34;, &#34;verbs&#34;: [ &#34;update&#34; ] }, { &#34;name&#34;: &#34;namespaces/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Namespace&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;nodes&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Node&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;no&#34; ], &#34;storageVersionHash&#34;: &#34;XwShjMxG9Fs=&#34; }, { &#34;name&#34;: &#34;nodes/proxy&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;NodeProxyOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;nodes/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;Node&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;persistentvolumeclaims&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;pvc&#34; ], &#34;storageVersionHash&#34;: &#34;QWTyNDq0dC4=&#34; }, { &#34;name&#34;: &#34;persistentvolumeclaims/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;persistentvolumes&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;PersistentVolume&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;pv&#34; ], &#34;storageVersionHash&#34;: &#34;HN/zwEC+JgM=&#34; }, { &#34;name&#34;: &#34;persistentvolumes/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: false, &#34;kind&#34;: &#34;PersistentVolume&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;pods&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Pod&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;po&#34; ], &#34;categories&#34;: [ &#34;all&#34; ], &#34;storageVersionHash&#34;: &#34;xPOwRZ+Yhw8=&#34; }, { &#34;name&#34;: &#34;pods/attach&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodAttachOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/binding&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Binding&#34;, &#34;verbs&#34;: [ &#34;create&#34; ] }, { &#34;name&#34;: &#34;pods/eviction&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;group&#34;: &#34;policy&#34;, &#34;version&#34;: &#34;v1beta1&#34;, &#34;kind&#34;: &#34;Eviction&#34;, &#34;verbs&#34;: [ &#34;create&#34; ] }, { &#34;name&#34;: &#34;pods/exec&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodExecOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/log&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Pod&#34;, &#34;verbs&#34;: [ &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/portforward&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodPortForwardOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;get&#34; ] }, { &#34;name&#34;: &#34;pods/proxy&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodProxyOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;pods/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Pod&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;podtemplates&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;PodTemplate&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;storageVersionHash&#34;: &#34;LIXB2x4IFpk=&#34; }, { &#34;name&#34;: &#34;replicationcontrollers&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ReplicationController&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;rc&#34; ], &#34;categories&#34;: [ &#34;all&#34; ], &#34;storageVersionHash&#34;: &#34;Jond2If31h0=&#34; }, { &#34;name&#34;: &#34;replicationcontrollers/scale&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;group&#34;: &#34;autoscaling&#34;, &#34;version&#34;: &#34;v1&#34;, &#34;kind&#34;: &#34;Scale&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;replicationcontrollers/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ReplicationController&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;resourcequotas&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ResourceQuota&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;quota&#34; ], &#34;storageVersionHash&#34;: &#34;8uhSgffRX6w=&#34; }, { &#34;name&#34;: &#34;resourcequotas/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ResourceQuota&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;secrets&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Secret&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;storageVersionHash&#34;: &#34;S6u1pOWzb84=&#34; }, { &#34;name&#34;: &#34;serviceaccounts&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ServiceAccount&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;deletecollection&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;sa&#34; ], &#34;storageVersionHash&#34;: &#34;pbx9ZvyFpBE=&#34; }, { &#34;name&#34;: &#34;services&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Service&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;list&#34;, &#34;patch&#34;, &#34;update&#34;, &#34;watch&#34; ], &#34;shortNames&#34;: [ &#34;svc&#34; ], &#34;categories&#34;: [ &#34;all&#34; ], &#34;storageVersionHash&#34;: &#34;0/CO1lhkEBI=&#34; }, { &#34;name&#34;: &#34;services/proxy&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;ServiceProxyOptions&#34;, &#34;verbs&#34;: [ &#34;create&#34;, &#34;delete&#34;, &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] }, { &#34;name&#34;: &#34;services/status&#34;, &#34;singularName&#34;: &#34;&#34;, &#34;namespaced&#34;: true, &#34;kind&#34;: &#34;Service&#34;, &#34;verbs&#34;: [ &#34;get&#34;, &#34;patch&#34;, &#34;update&#34; ] } ] } .",
            "url": "https://ntorba.github.io/writing/kubernetes/api/2020/07/27/exploring-kubernetes-api.html",
            "relUrl": "/kubernetes/api/2020/07/27/exploring-kubernetes-api.html",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Depth over Breadth",
            "content": "This is an idea I first explicitly learned about while listening to The Art of Learning by Josh Waitzkin. The main idea is that you can gain a much better understanding of the bigger picture (any object or field of study) by digging deep on a seemingly small part. For example, as a software engineer, depth over breadth would be dedicating your time to a single programming language instead of trying to learn 3 at once. A deep understanding of a single language improves your general programming ability more than a shallow understanding of many. Not to mention, learning new languages after a deep understanding of one opens the door to many important, nuanced connections. . Digging into the small part is what Josh calls the micro. “Depth over breadth” is understanding the macro (programming in general) from the micro (a single programming language). . A great example Josh uses in the book is from The Art of Motorcycle Maintenance. The main character, Phadreus, is a professor at a college in Bozeman, Montana. He teaches literature and writing. At one point, he has a notably hard working student completely stumped with writer’s block. The assignment is to write 500 words about Bozeman, a small town in rural Montana. Despite her determination, she just couldn’t get any words on the page. After many attempts to help, Phradreus frustratedly tells her “Narrow it down to the front of one building on the main street of Bozeman. The Opera House. Start with the upper left-hand brick.” The next day, the girl turned in a 5000 word essay. This point of view gave her endless inspiration. Focusing on something so small (the micro) gave her an entirely new view of the whole town (the macro). . This idea takes our original software analogy even further. Forget focusing on an entire language. Focus on a single library. Tear it apart. Use the debugger and step through all levels of the code. Look at how the authors abstracted their ideas. Analyze the data structures. Look for the use of language specific features. Contribute to it. . Deep understanding of a single library does much more than just help you understand that library. As you use others, you begin to see important connections, or striking differences. Those connections help you pick up new libraries much faster. You also begin to build a much deeper understanding of the language itself. . With a strategy like this, you build a deeper understanding of the macro (a programming language, or even programming itself) by narrowing in on the micro (a specific library of a single language). .",
            "url": "https://ntorba.github.io/writing/markdown/learning/learning%20strategies/growth/2020/07/24/depth-over-breadth.html",
            "relUrl": "/markdown/learning/learning%20strategies/growth/2020/07/24/depth-over-breadth.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Custom Resources and Operators",
            "content": "Intro . In this post we will walk through the basics behind the seldon custom resource definition. At a high level, kubernetes only job is to maintain the desired state of the cluster. All interactions are changes to the desired state. The magic of kubernetes is that once you tell it the new state, it creates and maintains that state for you. The seldon-core projects serves inferences graphs on kubernetes with a custom resource and operator. These provide automation of complex systems, while allowing us to easily configure the desired state of our deployments without doing a lot of manual work. . Launch Cluster . To get started, let&#39;s get a cluster up and running. If you followed part 1 of this series, you can do so with kind. Below, I create a cluster, create a namespace called seldon-intro, then use kubens to make seldon-into my default namespace (so I don&#39;t have to include it in every command). . !kind create cluster !kubectl create namespace seldon-intro !kubens seldon-intro . Creating cluster &#34;kind&#34; ... ✓ Ensuring node image (kindest/node:v1.17.0) 🖼7l ✓ Preparing nodes 📦 7l ✓ Writing configuration 📜7l ✓ Starting control-plane 🕹️7l ✓ Installing CNI 🔌7l ✓ Installing StorageClass 💾7l Set kubectl context to &#34;kind-kind&#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂 . KubeAPI, Custom Resources, and Operators . The KubeAPI is the medium through which all communication is handled in a kubernetes cluster. It is a rest server. When you send commands to a kubernetes cluster, you are hitting a specific api endpoint with commands for the server to execute. Kubernetes comes with some built-in objects you should be familiar with. Deployments, services, pods, etc. These objects are useful in and of themselves, but we often need to use many of them at once, which can get cumbersome. For those familiar with seldon, you know that you can create very complex inference graphs with many components. Instead of deploying and connecting all seldon services manually, we are able to build a single json/yaml configuration that deploys the entire graph. This is possible because of operators and custom resources. Operators and custom resources have an intimate relationship, and must be used in tandem. Earlier, I introduce the KubeAPI. We know that is how all internal and external communication is handled in the cluster. With operators, we extend the KubeAPI. In other words, operators allow us to add more endpoints to the KubeApi to carry out custom commands in our cluster. Which is where custom resources come in. They define the custom instructions for our new endpoint to execute. Let&#39;s see an operator and custom resource in action. . Install Seldon-core . I suggest using helm to install seldon-core. If you haven&#39;t used helm before, use this page to find install instructions. (If you&#39;re on a mac, just use brew install helm). For those familiar with python, helm is like the pip for kubernetes. It works by using helm charts. A chart is a group of files that describe a higher level application using built-in kubernetes resources. For example, you could use a helm chart to deploy a full stack web application. We are going to use helm to install seldon-core and seldon-core-operator. These helm charts are what will allow us to deploy seldon inference graphs. You can learn more about helm charts here. . Once helm is installed, use it to install seldon-core and seldon-core-operator with the following command: . !helm install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --set usageMetrics.enabled=true --set ambassador.enabled=true --namespace seldon-intro #unnecessary after using `kubens seldon-intro`, but keeping here to make sure the install is explicit . NAME: seldon-core LAST DEPLOYED: Mon Jul 27 08:32:08 2020 NAMESPACE: seldon-intro STATUS: deployed REVISION: 1 TEST SUITE: None . After a successful install of seldon-core and seldon-core-operator, run the following command: . !kubectl get deployments . NAME READY UP-TO-DATE AVAILABLE AGE seldon-controller-manager 1/1 1 1 6s . We see we now have a deployment, seldon-controller-manager, running in our seldon-intro namespace. . !!kubectl describe svc seldon-webhook-service . [&#39;Name: seldon-webhook-service&#39;, &#39;Namespace: seldon-intro&#39;, &#39;Labels: app=seldon&#39;, &#39; app.kubernetes.io/instance=seldon-core&#39;, &#39; app.kubernetes.io/managed-by=Helm&#39;, &#39; app.kubernetes.io/name=seldon-core-operator&#39;, &#39; app.kubernetes.io/version=1.2.1&#39;, &#39;Annotations: meta.helm.sh/release-name: seldon-core&#39;, &#39; meta.helm.sh/release-namespace: seldon-intro&#39;, &#39;Selector: app.kubernetes.io/instance=seldon1,app.kubernetes.io/name=seldon,app.kubernetes.io/version=v0.5,app=seldon,control-plane=seldon-controller-manager&#39;, &#39;Type: ClusterIP&#39;, &#39;IP: 10.96.113.42&#39;, &#39;Port: &lt;unset&gt; 443/TCP&#39;, &#39;TargetPort: 443/TCP&#39;, &#39;Endpoints: 10.244.0.5:443&#39;, &#39;Session Affinity: None&#39;, &#39;Events: &lt;none&gt;&#39;] . The docker image name used to create the seldon-controller-manager pod and deployment is called docker.io/seldonio/seldon-core-operator:1.2.1. That is because the seldon-controller-manager is the seldon-core-operator. . Be sure to match the last line with the namespace you created in the previous command. We will talk more about what the other lines mean later in the series. After installing, run . !kubectl get pods -o wide . and you&#39;ll see you have a pod running with seldon-controller-manager in the name (followed by a random string, we will talk about why that is soon). Next, run . !kubectl get deployments . Seldon-controller-manager is a kubernetes operator. As described here, kubernetes operators are extensions of the kubernetes api that allow users to easily package and deploy complex applications on kubernetes. The seldon-core-operator adds a lot of functionality to the kubernetes cluster and allows us to interact with seldon deployments as if they were a built-in kubernetes object. Along with this, we also installed the seldon-core custom resource definition. Kubernetes custom resources are extensions of the native kuberetes api. As you will see soon, we will now be able to deploy and interact with a new structure, the sdep, in the same fashion we deploy and monitor kubernetes deployments, servives, and other built-in resources. . All interactions with a kubernetes cluster is the user specifying a new desired state, and the kubernetes cluster using it&#39;s resources to change into that desired state. When, a custom resource is created, an operator is needed as well to tell kubernetes how to handle updates to the desired state of the custom resource. The seldon-core operator is what allows users to make edits to currently running seldon deployments without downtime. . Operators allows kubernetes to run stateful applications. A popular usecase for operators is databases. . To get a better idea at how helm charts add to your kubernetes cluster, check out the Exploring the Kubernetes API post digs into the basic internals of how your cluster actually receives commands. . In this post, they write . If you had to sum up Kubernetes in a word, the best choice might not be “orchestration” but “automation.” That’s what it’s all about:Kubernetes enables the automation of the infrastructure (and corresponding operational burden of managing that infrastructure) necessary for running containerized applications – a must when running these apps at scale in production environments. This is very evident with the Seldon deployment CRD and operator. As we move along and start to create our own seldon deployments, you will see the power of this automation. With a single file, a seldon deployment will launch multiple services, deployments, pods, and allow for continuous updates of all those components through the Kubernetes api. Seldon has leveraged the power of the automation offered by kubernetes to create inferences graphs of many components. . Another great quote from that article: . “Operators are simplifying the process highly complex distributed database management by defining the installation, scale, updates, and management lifecycle of a stateful clustered application,” says Yossi Jana, DevOps team leader at AllCloud. From another vantage point, consider life without Operators. “Without Operators, many applications need intervention to deploy, scale, reconfigure, upgrade, or recover from faults,” Thompson says. “If your app – or apps that you depend on, such as your database management system – [requires] DevOps engineers hovering over a keyboard in these critical moments, hoping they get the steps correctly, you’re almost certain to have greater downtime and more stress in your team.” . From kubernetes-operator-sdk tutorial, operators are used to define custom resources. They extend the kubernetes api to tell the cluster how to handle those resources. Operators themselves run in pods. This is why you see the seldon-controller-manager deployment and pod running after we install seldon-core with helm. . As described in the kubernetes docs here, operators follow the controller pattern, which means they are responsible for keep the desired state of the custom resource they are responsible for. The seldon-core-operator is responsbile for the Seldon Deployment custom resource. That means, when we create or edit a seldon deployment, the seldon-core-operator is responsible for adjusting the kubernetes deployment to the desired state to meet the new edits applied by the user. . In every article you read about kubernetes operators, you&#39;ll find some sentiment to the fact that is confusing the first time around. In fact, pretty much everything you learn about kubernetes will be confusing the first time around. Don&#39;t let that discourage you. That is why we are using seldon to help our understanding. Instead of reading description after description of what a custom resource and operator are, let&#39;s actually use them to begin to understand the power they provide. . Let&#39;s launch our first seldon deployment onto the cluster to see what the consequences of creating a seldon deployment custom resource are. . %%bash kubectl apply -f - &lt;&lt; END apiVersion: machinelearning.seldon.io/v1alpha2 kind: SeldonDeployment metadata: name: iris-model spec: name: sklearn-iris-deployment predictors: - componentSpecs: - spec: containers: - image: seldonio/sklearn-iris:0.1 imagePullPolicy: IfNotPresent name: sklearn-iris-classifier graph: children: [] endpoint: type: REST name: sklearn-iris-classifier type: MODEL name: predictor replicas: 1 END . !kubectl get pods .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/20/dive-into-operators-and-custom-resources.html",
            "relUrl": "/kubernetes/docker/2020/07/20/dive-into-operators-and-custom-resources.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Cluster Interaction Basics",
            "content": "Intro . This post walks through some kubernetes basics and launches your first deployment and service on your cluster. .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/18/cluster-interaction-basics.html",
            "relUrl": "/kubernetes/docker/2020/07/18/cluster-interaction-basics.html",
            "date": " • Jul 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Seldon Super Series",
            "content": "Intro . I work on a team building an ML platform to allow researchers to deploy, monitor, and iterate on production machine learning models. The platform leverages seldon-core, an open source platform to deploy scalable machine learning models on kubernetes. Kubernetes is an intimidating subject. From the control-plane, to pods, to volumes, to everything else, there is always more you didn’t even know you didn’t know. My journey to getting familiar with kubernetes was learning through seldon-core. I had never used kubernetes before, but to leverage seldon, it is necessary. Over the past few months I’ve become comfortable developing, debugging, and deploying projects on a kubernetes cluster. Seldon-core is a great project to build kubernetes knowledge around. It leverages some advanced kubernetes concepts under the hood, while remaining easy to use and powerful. Not to mention, it is much easier to build motivation around a subject when you are actually building, instead of just reading the documentation. . Goals . I am not a kubernetes expert, but I have become comfortable enough to be highly productive working with kubernetes. This comfort level was developed through muscle memory using common kubectl commands, learning the basics of kubernetes built-in objects vs custom resources, finding errors, and more. With that in mind, here are my goals: . Main goal: Give readers a clear path to the “highly productive with kubernetes” level. | Stretch goal: Have readers match my comfort level in only one week. The Seldon Super Series is built around these goals. To do this, I wrote this series to lay out the resources I found my most useful along with some posts I wrote to show lessons I learned through trial and error. | . Who’s it for? . This series is useful for anyone looking to build a deeper understanding of kubernetes. Seldon-core is just a great vehicle for this learning. If you are also interested in leveraging seldon, all the better! You can be a complete kubernetes beginner, or come with some familiarity. If you are already familiar with kubernetes, feel free to jump to posts further into the series that get into more complex material! All of the examples throughout this tutorial are written in python. Most of the code examples are straightforward, with the focus on kubernetes and seldon, so even if you have a different preferred language, you should be able to following along just as well. This series is useful for . How to get started . If you are completely new to kubernetes, there is no better place to start than the kubernetes interactive tutorial. I suggest following this tutorial all the way through before getting started on anything else. It requires no install, no setup, and introduces fundamentals used when working with any app deployed on a cluster. After that, I suggest following Launch a local kubernetes cluster to get a kubernetes cluster running on your local machine with kind. If that doesn’t work for you, there are many good resources available to get a cluster running on AWS or Google Cloud. Having a cluster to hack on is super imporant. You will not be able to build muscle memory without actually writing commands and debugging issues on your own setup. Once you have a cluster to hack on, get started with First Seldon Deployment. Even if you have previous seldon experience, I suggest starting, here, because many other posts will use this as a baseline to build on (don’t worry if you don’t start here, though, any post that does require some setup will make that clear). . Launch a local kubernetes cluster Get a local kubernetes cluster up and running so you can experiment locally! Important for those of us who don’t have easy access to a remote cluster. | . | First Seldon Deployment Deploy a model endpoint on kubernetes! | . | Multi-component Seldon Deployment Use multiple seldon-core components to deploy and inference graph! | . | Explore the Kubernetes API Use kubectl proxy to understand how to communicate with a kubernetes cluster! | . | Seldon-core Custom Resource and Operator. Create your first seldon deployment and read about what makes the seldon deployment custom resource so useful. Take a closer look at how seldon-core and seldon-core-operator extend the kubernetes api and make it easy to deploy seldon inference graphs! | . | Debugging Seldon Deployments. Take a look at where to find your logs and diagnosis some common issues. | Seldon-core analytics and load testing with Locust | Multi-pod Seldon Deployments. Deploy individual components of your inference graph in their own pods to define custom deployment specs! | . | Horizontal Pod Autoscaling Seldon Deployments Autoscaling your deployments! | . | CD with Argocd * Try out Argocd for continuous deployment! | Why Seldon-core? . Seldon-core is an open source projects built by the London based startup Seldon. With seldon-core, you can use python (and java) to easily deploy ML models built in any framework at scale. However, they offer tools for more than just model serving. With seldon-core, you construct an inference graph. The inference graph is built with these components: . Model | Transformer | Combiner | Router These additional components add the ability to create much more than just a single model. You can set up custom A/B tests, multi-armed bandits, scalable ensemble systems, and much more. In short, seldon-core inference graphs are powerful! | .",
            "url": "https://ntorba.github.io/writing/markdown/seldon/kubernetes/python/2020/07/17/seldon-super-series.html",
            "relUrl": "/markdown/seldon/kubernetes/python/2020/07/17/seldon-super-series.html",
            "date": " • Jul 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Local Kubernetes on KIND",
            "content": "Intro . Welcome to the first post of the Seldon Super Series! This post is for those who don&#39;t yet have access to a kubernetes cluster. We&#39;ll walkthrough how to use Kind to launch a cluster on you local machine! If you already have access to a kubernetes cluster, and also have kubectl installed, then move onto part 2! Otherwise, follow along here before you move on! . Reqs . None! This is the first post in the series! | . Goals . Launch a local kubernets cluster using kind, and install seldon on the cluster to allow you to follow along with the rest of the posts in this series | . Install kubectl . If this is the first time you&#39;ve used kubernetes, you will need to install kubectl, the command line tool for interacting with kubernetes. This can be downloaded here. On mac you can use brew install kubectl. Check your install by running: . !kubectl version --client --short . Client Version: v1.18.5 . You should see output similar to this. It shouldn&#39;t be a problem if you&#39;re version is a bit different than this. . Install Kind . If you&#39;re on mac, it&#39;s as simple as brew install kind. If not, check out this page . Create your First Cluster . Todo: ADD LOCAL REGISTRY. They need this for the following examples (or access to DockerHub) . !kind create cluster . Creating cluster &#34;kind&#34; ... ✓ Ensuring node image (kindest/node:v1.17.0) 🖼 ✓ Preparing nodes 📦 7l ✓ Writing configuration 📜7l ✓ Starting control-plane 🕹️7l ✓ Installing CNI 🔌7l ✓ Installing StorageClass 💾7l Set kubectl context to &#34;kind-kind&#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Thanks for using kind! 😊 . It&#39;s as simple as that. If it is your first time running kind, it will automatically download the appropiate docker image (something like kindest/node:1.17.0), which may take a few minutes. After that command is finished, check if your cluster is running: . !kubectl cluster-info . Kubernetes master is running at https://127.0.0.1:32771 KubeDNS is running at https://127.0.0.1:32771/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;. . If you see output like above, displaying info about your Kubernetes master and KubeDNS, then you have successfully launched a local kubernetes cluster! . Install Seldon-core . Because we will need seldon-core for all of the following posts, we will install it here. Anytime you need to re-launch a kind cluster to follow along the other posts, you will be able to run this notebook to get it back up and running. . To install seldon-core on the cluster, use helm. To install helm itself, find directions here, or use brew install helm on mac. . Once helm is installed, use it to install seldon-core and seldon-core-operator with the following command: . !helm install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --set usageMetrics.enabled=true --set ambassador.enabled=true --namespace seldon-intro . !kubectl get pods print(&quot;--&quot;) !kubectl get deployments . You should see a pod and deployment with seldon-controller-manager in the name. This pod and deployment house the seldon-core operator, extends the kubernetes api. For now, just confirming that pod is running is all we need. . Bonus: Install kubectx and kubens . As you follow through the next posts, I will be using the kubectx and kubens command line tools. If you are on mac, you can install them with brew: brew install kubectx. This will download and install both kubectx and kubens. If you&#39;re not on mac, find install instructions here. These allow you to easily switch between kubernetes contexts and namespaces. You can perform all the same actions with kubectl, but kubectx and kubens make some common commands much quicker. .",
            "url": "https://ntorba.github.io/writing/kubernetes/docker/2020/07/17/local-kubernetes-with-seldon.html",
            "relUrl": "/kubernetes/docker/2020/07/17/local-kubernetes-with-seldon.html",
            "date": " • Jul 17, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Good At Math Bad At Writing",
            "content": "Good at Math, Bad at Writing . The biggest failure of my education was the acceptance of the “good at math, bad at writing” label. It comes as no surprise that I ended up working as a software engineer, but it took until now to see how much time I wasted hating writing, when in fact, the underlying principles are strikingly similar. . Looking at the end product, the similarities are hard to see. Books and programs are much different, but the process to generate them is quite similar. For both, the ultimate goal is to give the user/reader (“user” for software and “reader” for writing - they serve the same purpose) a particular experience by providing information in a well thought out structure. This goal is achieved using a set of tools and conventions. Software leverages programming languages with different designs (object oriented vs functional, for example) and frameworks (web application vs data application). Writing leverages spoken language with accepted structures such as short-stories, books, long-form, and poems. In both, deciding the general structure and design (picking a programming language vs picking a writing structure) is integral to creating the end experience you wish to generate. . Software engineers specialize in particular niches similar to authors specializing in particular structures. I build data applications with python to improve the experience of data scientists while poets use poems to create an experience for their readers. In the end, the best software engineers and writers are those who most effectively organize and present necessary information to their audience. . It is sometimes easy to miss when this is done well, but never hard to miss when done poorly. Everyone knows the absolute frustration of using software that is particularly difficult to understand or flat out doesn’t do what its supposed to. While bad writing leaves you lost, or worse, leaves you in apparent understanding, only later finding you completely missed the point the author wanted to get across (something I’ve had happen in my writing many a time). Both of these are results of bad information organization. At some point the creators mis executed along their path to creating the experience they planned on. . Most disciplines share the same underlying principles. These hidden connections is what makes me so sad about the all too common “Good at math, bad at writing” label given to so many students at such a young age. It is harmful to plant this idea in a young mind, blocking them from the hidden connections they then don’t bother to look for. I’ve come to realize how important and enjoyable writing is in my everyday life. Not only do you need to write more than ever to communicate with work colleagues via slack and email, but nothing can help boost memory more than writing well thought out notes about new topics and ideas. . I’m angry at any teacher who allowed this idea to propagate or ever did a shitty job teaching an English class. English teachers have such a great opportunity to showcase how important writing is to every aspect of life and they squandered it. . This realization has been very important to me. I will forever despise the “good at math, bad at writing” because of the limitations it tricks people into thinking they have. .",
            "url": "https://ntorba.github.io/writing/2020/05/07/Good-at-Math-Bad-At-Writing.html",
            "relUrl": "/2020/05/07/Good-at-Math-Bad-At-Writing.html",
            "date": " • May 7, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Exploring Metaflow",
            "content": "About . metaflow is a python package open sourced by netflix to help data scientists easily scale their project workflows. Metaflow is mainly interacted with through decorators. In this post, we will get behind the scenes of how these decorators actually work. . The code . To start, let&#39;s take a look at the first example in the documentation. This is a simple flow. . from metaflow import FlowSpec, step class LinearFlow(FlowSpec): @step def start(self): self.my_var = &#39;hello world&#39; self.next(self.a) @step def a(self): print(&#39;the data artifact is: %s&#39; % self.my_var) self.next(self.end) @step def end(self): print(&#39;the data artifact is still: %s&#39; % self.my_var) LinearFlow() . We see that the LinearFlow python class inherits from metaflow&#39;s FlowSpec class, and each of the functions are decorated with @step. As seen (here)[https://docs.metaflow.org/metaflow/basics], this basic flow follows metaflow&#39;s guidelines. However, what is actually happening? How does it turn our functions into pipeline steps? Let&#39;s start by taking a look at the Flowspec class. . (Flowspec)[https://github.com/Netflix/metaflow/blob/master/metaflow/flowspec.py] definition and constructor. Full code can be found at the link. . class FlowSpec(object): &quot;&quot;&quot; Main class from which all Flows should inherit. Attributes - script_name index input &quot;&quot;&quot; # Attributes that are not saved in the datastore when checkpointing. # Name starting with &#39;__&#39;, methods, functions and Parameters do not need # to be listed. _EPHEMERAL = {&#39;_EPHEMERAL&#39;, &#39;_datastore&#39;, &#39;_cached_input&#39;, &#39;_graph&#39;, &#39;_flow_decorators&#39;, &#39;_steps&#39;, &#39;index&#39;, &#39;input&#39;} _flow_decorators = {} def __init__(self, use_cli=True): &quot;&quot;&quot; Construct a FlowSpec Parameters - use_cli : bool, optional, default: True Set to True if the flow is invoked from __main__ or the command line &quot;&quot;&quot; self.name = self.__class__.__name__ self._datastore = None self._transition = None self._cached_input = {} self._graph = FlowGraph(self.__class__) self._steps = [getattr(self, node.name) for node in self._graph] if use_cli: # we import cli here to make sure custom parameters in # args.py get fully evaluated before cli.py is imported. from . import cli cli.main(self) .",
            "url": "https://ntorba.github.io/writing/jupyter/2020/03/08/metaflow-exploration.html",
            "relUrl": "/jupyter/2020/03/08/metaflow-exploration.html",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Figuring Out The Brain",
            "content": "Intro . Mathew Cobb recently wrote Why your brain is not a computer on The Guardian. Thank you to Data Science Weekly and Data Exlixir newsletters for sending both sending this to me. . Summary . In this article, Cobb details some of history of the “brain as a computer metaphor.” With the rise of AI, this topic has become very relevant. There are many smart people in many different camps, and its hard to figure out who has the best information when there is so much out there. . My Thoughts . While reading this, an idea from a software engineering article I recently read struck me: “It is easier to write code than to read it.” The consequences of this fact are littered throughout code bases and can help us understand our struggle to understand the brain. . Reading code is a form of reverse engineering. Working in the reverse direction is often much more difficult than building from the ground up. This is counter-intuitive until you experience it. Very commonly in software, problems that seem quicky and easy end up thorny and difficult. So, a motivated engineer sees a big jumble of old code for an “easy” problem and decides this other person must have been an idiot, so they decide to take their own stab at it, only to find that their new solution is a new fangled mess as well. . This situaiton is prominent in other work as well. Many tasks are forms of reverse engineering - reading even! It is easier to write and understand your own thoughts than to read and understand someone else’s. So, if we humans find reverse engineering so difficult for these everyday tasks, its not wonder its been such a nightmare trying to reverse engineer the brain (I guess it’s a shame evolution isn’t a better author). It is no wonder our main metaphor for the brain is computers. We built computers and use them everyday. They are the closest analogue we have, although they are quite different. . This line of thought puts me firmly in the “better metaphor for understanding” category. The only chance humans have to understand the brain is to continue to try to build our own version then compare the outcome. . If we were given a modern computer 100 years ago, would it have advanced the rate of technology? I suppose not. It would have been so advanced that the people of the times wouldn’t have known where to start reverse engineering such a complex technology. So it is with the brain. It has somehow put itself far ahead of our time, out of reach of reverse engineering, a skill we so desperately wish we were proficient at for this task. .",
            "url": "https://ntorba.github.io/writing/markdown/philosophy/first%20draft/2020/03/07/figuring-out-the-brain.html",
            "relUrl": "/markdown/philosophy/first%20draft/2020/03/07/figuring-out-the-brain.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://ntorba.github.io/writing/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 . Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ntorba.github.io/writing/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "About Me . Trying to get better at writing because it seems like a good thing to get good at. .",
          "url": "https://ntorba.github.io/writing/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}